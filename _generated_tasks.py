"""
Auto-generated Flyte task definitions for the SAM2 → VideoPainter → Alpamayo pipeline.
Generated by following the _generated_tasks pattern - DO NOT EDIT MANUALLY
Mode: REMOTE
"""

import os
import logging
from typing import List, Optional

from hlx.wf import workflow, task, DedicatedNode, Node
from hlx.wf.mounts import MOUNTPOINT, FuseBucket

logger = logging.getLogger(__name__)


# Explicitly control what gets exported with "from _generated_tasks import *"
# This prevents shadowing hlx.wf decorators in importing modules
__all__ = []  # Will be populated at the end of the file


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------
NUM_GPUS = 1

# GCS bucket shared by all three stages
BUCKET = "mbadas-sandbox-research-9bb9c7f"

# Output base path used by the master pipeline
OUTPUT_BASE = (
    "gs://mbadas-sandbox-research-9bb9c7f/workspace/user/hbaskar/outputs"
)


# ---------------------------------------------------------------------------
# Container images
# ---------------------------------------------------------------------------
SAM2_CONTAINER_IMAGE = os.environ.get(
    "SAM2_CONTAINER_IMAGE",
    "europe-west4-docker.pkg.dev/mb-adas-2015-p-a4db/research/harimt_sam2:latest",
)

# VP image suffix (optional tag from build_and_run.sh)
_VP_RUN_SUFFIX = os.environ.get("VP_RUN_SUFFIX", "").strip()
_VP_REMOTE_BASE = (
    f"europe-west4-docker.pkg.dev/mb-adas-2015-p-a4db/research/harimt_vp{_VP_RUN_SUFFIX}"
    if _VP_RUN_SUFFIX
    else "europe-west4-docker.pkg.dev/mb-adas-2015-p-a4db/research/harimt_vp"
)
VP_CONTAINER_IMAGE = os.environ.get("VP_CONTAINER_IMAGE", f"{_VP_REMOTE_BASE}:latest")

ALPAMAYO_CONTAINER_IMAGE = os.environ.get(
    "ALPAMAYO_CONTAINER_IMAGE",
    "europe-west4-docker.pkg.dev/mb-adas-2015-p-a4db/research/alpamayo_vla:latest",
)


# ---------------------------------------------------------------------------
# FuseBucket factory
# ---------------------------------------------------------------------------
def create_fuse_buckets():
    """Create FuseBucket configurations for all three pipeline stages."""
    return {
        # ── SAM2 ────────────────────────────────────────────────────────────
        "sam2_checkpoints": FuseBucket(
            bucket=BUCKET,
            name="sam2-checkpoints",
            prefix="workspace/user/hbaskar/Video_inpainting/sam2_checkpoint",
        ),
        "sam2_input_videos": FuseBucket(
            bucket=BUCKET,
            name="input-videos",
            prefix=os.environ.get(
                "SAM2_INPUT_PREFIX",
                "workspace/user/hbaskar/Input/data_physical_ai",
            ),
        ),

        # ── VideoPainter ────────────────────────────────────────────────────
        "vp_bucket": FuseBucket(
            bucket=BUCKET,
            name="vp-bucket",
            prefix="workspace/user/hbaskar/Video_inpainting/videopainter",
        ),
        "vp_data": FuseBucket(
            bucket=BUCKET,
            name="data",
            prefix="workspace/user/hbaskar/outputs/preprocessed_data_vp",
        ),
        "vp_trained_fluxfill": FuseBucket(
            bucket=BUCKET,
            name="vp-trained-fluxfill",
            prefix=os.environ.get(
                "TRAINED_FLUXFILL_GCS_PATH",
                "workspace/user/hbaskar/Video_inpainting/videopainter/training/trained_checkpoint/fluxfill_single_white_solid_clearroad_20260212_151908",
            ),
        ),
        "vp_vlm_7b": FuseBucket(
            bucket=BUCKET,
            name="vp-vlm-7b",
            prefix="workspace/user/hbaskar/Video_inpainting/videopainter/ckpt/vlm/Qwen2.5-VL-7B-Instruct",
        ),

        # ── Alpamayo ────────────────────────────────────────────────────────
        "alpamayo_checkpoints": FuseBucket(
            bucket=BUCKET,
            name="alpamayo-ckpt",
            prefix="workspace/user/hbaskar/Video_inpainting/vla/alpamayo/checkpoints",
        ),
        "alpamayo_video_data": FuseBucket(
            bucket=BUCKET,
            name="alpamayo-video-data",
            prefix=os.environ.get(
                "ALPAMAYO_VIDEO_DATA_PREFIX",
                "workspace/user/hbaskar/outputs/vp",
            ),
        ),
        "alpamayo_output": FuseBucket(
            bucket=BUCKET,
            name="alpamayo-output",
            prefix=os.environ.get(
                "ALPAMAYO_OUTPUT_BASE",
                "workspace/user/hbaskar/outputs/alpamayo",
            ),
        ),
    }


BUCKETS = create_fuse_buckets()


# ---------------------------------------------------------------------------
# SAM2 defaults
# ---------------------------------------------------------------------------
SAM2_BASE_WORKDIR = "/workspace/sam2"
SAM2_DEFAULT_CHECKPOINT = os.path.join(SAM2_BASE_WORKDIR, "checkpoints", "sam2.1_hiera_large.pt")
SAM2_DEFAULT_CONFIG = "configs/sam2.1/sam2.1_hiera_l.yaml"
SAM2_OUTPUT_BUCKET_BASE = os.environ.get(
    "SAM2_OUTPUT_BASE",
    "gs://mbadas-sandbox-research-9bb9c7f/workspace/user/hbaskar/outputs/sam2",
)
SAM2_PREPROCESSED_BUCKET_BASE = os.environ.get(
    "SAM2_PREPROCESSED_OUTPUT_BASE",
    "gs://mbadas-sandbox-research-9bb9c7f/workspace/user/hbaskar/outputs/preprocessed_data_vp",
)

# Chunk-based video selection defaults
SAM2_INPUT_BASE = os.environ.get(
    "SAM2_INPUT_BASE",
    f"gs://{BUCKET}/workspace/user/hbaskar/Input/data_physical_ai/camera_front_tele_30fov",
)
SAM2_DEFAULT_CHUNK_FROM = int(os.environ.get("SAM2_CHUNK_START", "0"))
SAM2_DEFAULT_CHUNK_TO = int(os.environ.get("SAM2_CHUNK_END", "19"))
SAM2_DEFAULT_FILES_PER_CHUNK = int(os.environ.get("SAM2_FILES_PER_CHUNK", "1"))

# Build a chunks:// URI from chunk parameters – resolved at runtime inside the GPU container
_SAM2_INPUT_BASE_NO_GS = SAM2_INPUT_BASE.replace("gs://", "", 1)
SAM2_DEFAULT_VIDEO_URI = (
    f"chunks://{_SAM2_INPUT_BASE_NO_GS}"
    f"?start={SAM2_DEFAULT_CHUNK_FROM}&end={SAM2_DEFAULT_CHUNK_TO}"
    f"&per_chunk={SAM2_DEFAULT_FILES_PER_CHUNK}"
)


# ---------------------------------------------------------------------------
# VideoPainter defaults
# ---------------------------------------------------------------------------
VP_BASE_WORKDIR = "/workspace/VideoPainter"
VP_DEFAULT_MODEL_PATH = os.path.join(VP_BASE_WORKDIR, "ckpt", "CogVideoX-5b-I2V")
VP_DEFAULT_BRANCH_PATH = os.path.join(VP_BASE_WORKDIR, "ckpt", "VideoPainter/checkpoints/branch")
VP_DEFAULT_IMG_INPAINT_PATH = os.path.join(VP_BASE_WORKDIR, "ckpt", "flux_inp")
VP_DEFAULT_DATA_RUN_ID = os.environ.get("DATA_RUN_ID", "001")
VP_DEFAULT_LLM_MODEL = "/workspace/VideoPainter/ckpt/vlm/Qwen2.5-VL-7B-Instruct"
VP_DEFAULT_LORA_PATH = "/workspace/VideoPainter/ckpt/trained_fluxfill_lora"
VP_FLUX_DEVICE_DEFAULT = "cuda:0"


# ---------------------------------------------------------------------------
# Alpamayo defaults
# ---------------------------------------------------------------------------
ALPAMAYO_BASE_WORKDIR = "/workspace/alpamayo"
ALPAMAYO_DEFAULT_MODEL_ID = os.environ.get(
    "MODEL_ID", "/workspace/alpamayo/checkpoints/alpamayo-r1-10b"
)


# ═══════════════════════════════════════════════════════════════════════════
# Stage 1: SAM2 Segmentation Tasks
# ═══════════════════════════════════════════════════════════════════════════

@task(
    compute=DedicatedNode(
        node=Node.A100_80GB_1GPU,
        ephemeral_storage="max",
        max_duration="3d",
    ),
    container_image=SAM2_CONTAINER_IMAGE,
    environment={"PYTHONUNBUFFERED": "1"},
    mounts=[BUCKETS["sam2_checkpoints"], BUCKETS["sam2_input_videos"]],
)
def task_sam2_segmentation(
    run_id: str,
    video_uris: Optional[List[str]] = None,
    checkpoint_path: str = SAM2_DEFAULT_CHECKPOINT,
    model_config: str = SAM2_DEFAULT_CONFIG,
    upload_to_gcp: bool = True,
    upload_to_local: bool = False,
    max_frames: int = 150,
) -> str:
    """Execute SAM2 video segmentation on multiple videos."""
    import subprocess
    import sys
    from datetime import datetime
    from pathlib import Path

    from hlx.wf.mounts import MOUNTPOINT
    from hlx.wf import fuse_prefetch_metadata

    if video_uris is None:
        video_uris = [SAM2_DEFAULT_VIDEO_URI]

    # ── Resolve video URIs from various input formats ───────────────────────
    def _resolve_video_uris(uris):
        """Resolve chunk specs, GCS folders, or pass-through file lists."""
        import subprocess as _sp
        from urllib.parse import urlparse, parse_qs

        if len(uris) == 1 and uris[0].startswith("chunks://"):
            # Format: chunks://<bucket>/<path>?start=N&end=M&per_chunk=K
            raw = uris[0]
            parsed = urlparse(raw)
            base_path = f"gs://{parsed.netloc}{parsed.path}".rstrip("/")
            params = parse_qs(parsed.query)
            chunk_start = int(params.get("start", ["0"])[0])
            chunk_end = int(params.get("end", ["19"])[0])
            per_chunk = int(params.get("per_chunk", ["1"])[0])

            logger.info(
                "Resolving chunks %d-%d (%d files/chunk) from %s",
                chunk_start, chunk_end, per_chunk, base_path,
            )
            resolved = []
            for ci in range(chunk_start, chunk_end + 1):
                chunk_folder = f"{base_path}/chunk_{ci:04d}/"
                ls = _sp.run(
                    ["gsutil", "ls", chunk_folder],
                    capture_output=True, text=True,
                )
                if ls.returncode != 0:
                    logger.warning("gsutil ls failed for %s: %s", chunk_folder, ls.stderr.strip())
                    continue
                mp4s = sorted(
                    line.strip()
                    for line in ls.stdout.splitlines()
                    if line.strip().endswith(".mp4")
                )
                if not mp4s:
                    logger.warning("No .mp4 files in %s", chunk_folder)
                    continue
                resolved.extend(mp4s[:per_chunk])
            if not resolved:
                raise ValueError(
                    f"No .mp4 files found in chunks {chunk_start}-{chunk_end} of {base_path}"
                )
            logger.info("Resolved %d videos from %d chunks", len(resolved), chunk_end - chunk_start + 1)
            return resolved

        if len(uris) == 1 and uris[0].rstrip("/").startswith("gs://") and uris[0].endswith("/"):
            # Single GCS folder
            folder = uris[0]
            logger.info("Resolving video folder: %s", folder)
            ls = _sp.run(["gsutil", "ls", folder], capture_output=True, text=True)
            if ls.returncode != 0:
                raise RuntimeError(f"gsutil ls failed for {folder}: {ls.stderr}")
            mp4s = sorted(
                line.strip()
                for line in ls.stdout.splitlines()
                if line.strip().endswith(".mp4")
            )
            if not mp4s:
                raise ValueError(f"No .mp4 files found in {folder}")
            logger.info("Found %d videos in folder", len(mp4s))
            return mp4s

        # Already individual file URIs — pass through
        return uris

    video_uris = _resolve_video_uris(video_uris)

    output_bucket = f"{SAM2_OUTPUT_BUCKET_BASE}/{run_id}"
    preprocessed_bucket = f"{SAM2_PREPROCESSED_BUCKET_BASE}/{run_id}"

    logger.info("=" * 60)
    logger.info("SAM2 Segmentation Task")
    logger.info(f"Processing {len(video_uris)} videos")
    logger.info(f"RUN_ID: {run_id}")
    logger.info(f"Output bucket: {output_bucket}")
    logger.info("=" * 60)

    # Prefetch mounted checkpoints
    SAM2_FUSE_MOUNT_ROOT = os.path.join(MOUNTPOINT, "sam2-checkpoints")
    INPUT_VIDEOS_MOUNT_ROOT = os.path.join(MOUNTPOINT, "input-videos")

    if checkpoint_path == SAM2_DEFAULT_CHECKPOINT:
        try:
            fuse_prefetch_metadata(SAM2_FUSE_MOUNT_ROOT)
        except Exception as e:
            logger.warning(f"Checkpoint prefetch failed (non-fatal): {e}")

        local_checkpoint_dir = os.path.join(SAM2_BASE_WORKDIR, "checkpoints")
        mounted_checkpoint_dir = os.path.join(SAM2_FUSE_MOUNT_ROOT, "checkpoints")
        if os.path.exists(mounted_checkpoint_dir):
            _ensure_symlink(mounted_checkpoint_dir, local_checkpoint_dir)

    # Verify checkpoint
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"SAM2 checkpoint not found: {checkpoint_path}")

    # Setup input video symlinks
    # The FuseBucket now mounts data_physical_ai, so camera subfolder is inside the mount
    CAMERA_SUBFOLDER = os.environ.get("SAM2_CAMERA_SUBFOLDER", "camera_front_tele_30fov")
    try:
        fuse_prefetch_metadata(INPUT_VIDEOS_MOUNT_ROOT)
    except Exception as e:
        logger.warning(f"Video mount prefetch failed (non-fatal): {e}")

    video_cache_dir = Path("/tmp/sam2_video_cache")
    video_cache_dir.mkdir(parents=True, exist_ok=True)

    local_video_paths = []
    for uri in video_uris:
        video_filename = uri.split("/")[-1]
        # Try with camera subfolder first (new layout)
        mounted_video_path = os.path.join(INPUT_VIDEOS_MOUNT_ROOT, CAMERA_SUBFOLDER, video_filename)
        # Fallback: direct under mount root (old layout)
        mounted_video_path_flat = os.path.join(INPUT_VIDEOS_MOUNT_ROOT, video_filename)
        local_video_path = video_cache_dir / video_filename
        if os.path.exists(mounted_video_path):
            if not local_video_path.exists():
                os.symlink(mounted_video_path, local_video_path)
            local_video_paths.append(str(local_video_path))
        elif os.path.exists(mounted_video_path_flat):
            if not local_video_path.exists():
                os.symlink(mounted_video_path_flat, local_video_path)
            local_video_paths.append(str(local_video_path))
        else:
            local_video_paths.append(uri)

    video_uris = local_video_paths

    # Run processing script
    script_path = os.path.join(SAM2_BASE_WORKDIR, "process_vide_sam2_hlxwf.py")
    cmd = [
        sys.executable,
        script_path,
        "--video-uris", *video_uris,
        "--checkpoint", checkpoint_path,
        "--model-cfg", model_config,
        "--output-bucket", output_bucket,
        "--preprocessed-bucket", preprocessed_bucket,
        "--max-frames", str(max_frames),
        "--run-id", run_id,
    ]
    if upload_to_gcp:
        cmd.append("--upload-gcp")
    if upload_to_local:
        cmd.append("--upload-local")

    result = subprocess.run(cmd, capture_output=True, text=True, cwd=SAM2_BASE_WORKDIR)

    if result.stdout:
        logger.info("STDOUT:\n%s", result.stdout)
    if result.stderr:
        logger.warning("STDERR:\n%s", result.stderr)
    if result.returncode != 0:
        raise RuntimeError(
            f"SAM2 failed (exit {result.returncode}).\n"
            f"stdout tail: {(result.stdout or '')[-4000:]}\n"
            f"stderr tail: {(result.stderr or '')[-4000:]}"
        )

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return (
        f"SAM2 complete – {len(video_uris)} videos, ts={timestamp}\n"
        f"Raw outputs: {output_bucket}/\n"
        f"VP format: {preprocessed_bucket}/"
    )


@task(
    compute=DedicatedNode(
        node=Node.A100_80GB_1GPU,
        ephemeral_storage="max",
        max_duration="3d",
    ),
    container_image=SAM2_CONTAINER_IMAGE,
    environment={"PYTHONUNBUFFERED": "1"},
    mounts=[BUCKETS["sam2_checkpoints"], BUCKETS["sam2_input_videos"]],
)
def task_sam2_single_video(
    run_id: str,
    video_uri: str,
    checkpoint_path: str = SAM2_DEFAULT_CHECKPOINT,
    model_config: str = SAM2_DEFAULT_CONFIG,
    upload_to_gcp: bool = True,
    upload_to_local: bool = False,
    max_frames: int = 150,
) -> str:
    """Execute SAM2 segmentation on a single video."""
    return task_sam2_segmentation(
        run_id=run_id,
        video_uris=[video_uri],
        checkpoint_path=checkpoint_path,
        model_config=model_config,
        upload_to_gcp=upload_to_gcp,
        upload_to_local=upload_to_local,
        max_frames=max_frames,
    )


# ═══════════════════════════════════════════════════════════════════════════
# Stage 2: VideoPainter Editing Tasks
# ═══════════════════════════════════════════════════════════════════════════

@task(
    compute=DedicatedNode(
        node=Node.A100_80GB_1GPU,
        ephemeral_storage="max",
        max_duration="3d",
    ),
    container_image=VP_CONTAINER_IMAGE,
    environment={
        "PYTHONUNBUFFERED": "1",
        "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:True",
        "VP_COG_DEVICE": "cuda:0",
        "VP_FLUX_DEVICE": VP_FLUX_DEVICE_DEFAULT,
        "VP_QWEN_DEVICE": "auto",
        "VP_UNLOAD_QWEN_AFTER_USE": "1",
    },
    mounts=[
        BUCKETS["vp_bucket"],
        BUCKETS["vp_data"],
        BUCKETS["vp_trained_fluxfill"],
        BUCKETS["vp_vlm_7b"],
    ],
)
def task_videopainter_edit_many(
    data_run_id: str = VP_DEFAULT_DATA_RUN_ID,
    output_run_id: Optional[str] = None,
    data_video_ids: str = "auto",
    data_video_id: Optional[str] = None,
    inpainting_sample_id: int = 0,
    prompt: str = "",
    model_path: str = VP_DEFAULT_MODEL_PATH,
    inpainting_branch: str = VP_DEFAULT_BRANCH_PATH,
    img_inpainting_model: str = VP_DEFAULT_IMG_INPAINT_PATH,
    img_inpainting_lora_path: str = VP_DEFAULT_LORA_PATH,
    img_inpainting_lora_scale: float = 0.0,
    output_name_suffix: str = "vp_edit_sample0.mp4",
    num_inference_steps: int = 70,
    guidance_scale: float = 6.0,
    num_videos_per_prompt: int = 1,
    dtype: str = "bfloat16",
    inpainting_frames: int = 49,
    down_sample_fps: int = 8,
    overlap_frames: int = 0,
    prev_clip_weight: float = 0.0,
    strength: float = 1.0,
    lane_count: str = "",
    lane_color: str = "",
    lane_pattern: str = "",
    lane_specs: str = "",
    video_editing_instruction: str = "auto",
    video_editing_instructions: str = "",
    llm_model: str = VP_DEFAULT_LLM_MODEL,
    dilate_size: int = 24,
    mask_feather: int = 8,
    caption_refine_iters: int = 10,
    caption_refine_temperature: float = 0.1,
    keep_masked_pixels: bool = True,
    seed: int = 42,
) -> str:
    """Execute VideoPainter editing on multiple videos.

    Delegates to the real run_videopainter_edit_many logic inside the container.
    This task is the self-contained, HLX-schedulable unit.
    """
    # The heavy logic lives inside the VP container image at
    # /workspace/VideoPainter.  We import and call it directly so that
    # all the staging, inference, evaluation, and upload code runs
    # inside the GPU container exactly as the original workflow.py does.
    import sys as _sys
    _sys.path.insert(0, VP_BASE_WORKDIR)

    from workflow import run_videopainter_edit_many as _run

    return _run(
        data_run_id=data_run_id,
        output_run_id=output_run_id,
        data_video_ids=data_video_ids,
        data_video_id=data_video_id,
        inpainting_sample_id=inpainting_sample_id,
        prompt=prompt,
        model_path=model_path,
        inpainting_branch=inpainting_branch,
        img_inpainting_model=img_inpainting_model,
        img_inpainting_lora_path=img_inpainting_lora_path,
        img_inpainting_lora_scale=img_inpainting_lora_scale,
        output_name_suffix=output_name_suffix,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        num_videos_per_prompt=num_videos_per_prompt,
        dtype=dtype,
        inpainting_frames=inpainting_frames,
        down_sample_fps=down_sample_fps,
        overlap_frames=overlap_frames,
        prev_clip_weight=prev_clip_weight,
        strength=strength,
        lane_count=lane_count,
        lane_color=lane_color,
        lane_pattern=lane_pattern,
        lane_specs=lane_specs,
        video_editing_instruction=video_editing_instruction,
        video_editing_instructions=video_editing_instructions,
        llm_model=llm_model,
        dilate_size=dilate_size,
        mask_feather=mask_feather,
        caption_refine_iters=caption_refine_iters,
        caption_refine_temperature=caption_refine_temperature,
        keep_masked_pixels=keep_masked_pixels,
        seed=seed,
    )


# ═══════════════════════════════════════════════════════════════════════════
# Stage 3: Alpamayo VLA Inference Tasks
# ═══════════════════════════════════════════════════════════════════════════

@task(
    compute=DedicatedNode(
        node=Node.A100_80GB_1GPU,
        ephemeral_storage="max",
        max_duration="3d",
    ),
    container_image=ALPAMAYO_CONTAINER_IMAGE,
    environment={
        "PYTHONUNBUFFERED": "1",
        "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:True",
        "HF_HOME": "/root/.cache/huggingface",
        "HF_TOKEN": os.environ.get("HF_TOKEN", ""),
    },
    mounts=[
        BUCKETS["alpamayo_checkpoints"],
        BUCKETS["alpamayo_video_data"],
        BUCKETS["alpamayo_output"],
    ],
)
def task_alpamayo_inference(
    video_data_gcs_path: str,
    output_run_id: str,
    model_id: str = ALPAMAYO_DEFAULT_MODEL_ID,
    num_traj_samples: int = 1,
) -> dict:
    """Execute Alpamayo VLA inference on video data.

    Delegates to the real run_alpamayo_inference_task logic inside the container.
    """
    import sys as _sys
    _sys.path.insert(0, ALPAMAYO_BASE_WORKDIR)

    from workflow import run_alpamayo_inference_task as _run

    return _run(
        video_data_gcs_path=video_data_gcs_path,
        output_run_id=output_run_id,
        model_id=model_id,
        num_traj_samples=num_traj_samples,
    )


# ---------------------------------------------------------------------------
# Helper used by SAM2 task body
# ---------------------------------------------------------------------------
def _ensure_symlink(src: str, dest: str) -> None:
    """Create a symlink from dest -> src if not already present."""
    from pathlib import Path as _Path
    import shutil

    dest_parent = _Path(dest).parent
    dest_parent.mkdir(parents=True, exist_ok=True)
    if os.path.islink(dest):
        if os.readlink(dest) == src:
            return
        os.unlink(dest)
    elif os.path.exists(dest):
        if os.path.isdir(dest):
            try:
                shutil.rmtree(dest)
            except OSError:
                return
        else:
            return
    os.symlink(src, dest)


# ═══════════════════════════════════════════════════════════════════════════
# Task registries
# ═══════════════════════════════════════════════════════════════════════════

TASKS_BY_CONFIG = {
    "sam2_segmentation": task_sam2_segmentation,
    "sam2_single_video": task_sam2_single_video,
    "videopainter_edit_many": task_videopainter_edit_many,
    "alpamayo_inference": task_alpamayo_inference,
}

TASKS_BY_STAGE = {
    "sam2": {
        "sam2_segmentation": task_sam2_segmentation,
        "sam2_single_video": task_sam2_single_video,
    },
    "videopainter": {
        "videopainter_edit_many": task_videopainter_edit_many,
    },
    "alpamayo": {
        "alpamayo_inference": task_alpamayo_inference,
    },
}


# ═══════════════════════════════════════════════════════════════════════════
# Individual-stage workflows
# ═══════════════════════════════════════════════════════════════════════════

@workflow
def eval_sam2_segmentation(
    run_id: str = "001",
    max_frames: int = 150,
) -> str:
    """Run SAM2 segmentation with default video list."""
    return task_sam2_segmentation(
        run_id=run_id,
        max_frames=max_frames,
    )


@workflow
def eval_sam2_single_video(
    run_id: str = "001",
    video_uri: str = SAM2_DEFAULT_VIDEO_URI,
    max_frames: int = 150,
) -> str:
    """Run SAM2 segmentation on a single video."""
    return task_sam2_single_video(
        run_id=run_id,
        video_uri=video_uri,
        max_frames=max_frames,
    )


@workflow
def eval_videopainter_edit_many(
    data_run_id: str = VP_DEFAULT_DATA_RUN_ID,
    output_run_id: Optional[str] = None,
    data_video_ids: str = "auto",
    inpainting_sample_id: int = 0,
    prompt: str = "",
    model_path: str = VP_DEFAULT_MODEL_PATH,
    inpainting_branch: str = VP_DEFAULT_BRANCH_PATH,
    img_inpainting_model: str = VP_DEFAULT_IMG_INPAINT_PATH,
    img_inpainting_lora_path: str = VP_DEFAULT_LORA_PATH,
    img_inpainting_lora_scale: float = 0.0,
    output_name_suffix: str = "vp_edit_sample0.mp4",
    num_inference_steps: int = 70,
    guidance_scale: float = 6.0,
    num_videos_per_prompt: int = 1,
    strength: float = 1.0,
    down_sample_fps: int = 8,
    inpainting_frames: int = 49,
    video_editing_instructions: str = "",
    llm_model: str = VP_DEFAULT_LLM_MODEL,
    caption_refine_iters: int = 10,
    caption_refine_temperature: float = 0.1,
    dilate_size: int = 24,
    mask_feather: int = 8,
    keep_masked_pixels: bool = True,
    seed: int = 42,
) -> str:
    """Run VideoPainter editing with production defaults from build_and_run.sh."""
    return task_videopainter_edit_many(
        data_run_id=data_run_id,
        output_run_id=output_run_id,
        data_video_ids=data_video_ids,
        inpainting_sample_id=inpainting_sample_id,
        prompt=prompt,
        model_path=model_path,
        inpainting_branch=inpainting_branch,
        img_inpainting_model=img_inpainting_model,
        img_inpainting_lora_path=img_inpainting_lora_path,
        img_inpainting_lora_scale=img_inpainting_lora_scale,
        output_name_suffix=output_name_suffix,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        num_videos_per_prompt=num_videos_per_prompt,
        strength=strength,
        down_sample_fps=down_sample_fps,
        inpainting_frames=inpainting_frames,
        video_editing_instructions=video_editing_instructions,
        llm_model=llm_model,
        caption_refine_iters=caption_refine_iters,
        caption_refine_temperature=caption_refine_temperature,
        dilate_size=dilate_size,
        mask_feather=mask_feather,
        keep_masked_pixels=keep_masked_pixels,
        seed=seed,
    )


@workflow
def eval_alpamayo_inference(
    video_data_gcs_path: str = f"gs://{BUCKET}/{os.environ.get('ALPAMAYO_VIDEO_DATA_PREFIX', 'workspace/user/hbaskar/outputs/vp')}",
    output_run_id: str = "001",
    model_id: str = ALPAMAYO_DEFAULT_MODEL_ID,
    num_traj_samples: int = 1,
) -> dict:
    """Run Alpamayo VLA inference with defaults."""
    return task_alpamayo_inference(
        video_data_gcs_path=video_data_gcs_path,
        output_run_id=output_run_id,
        model_id=model_id,
        num_traj_samples=num_traj_samples,
    )


# ═══════════════════════════════════════════════════════════════════════════
# Stage-level workflows
# ═══════════════════════════════════════════════════════════════════════════

@workflow
def eval_stage_sam2(
    run_id: str = "001",
    max_frames: int = 150,
) -> str:
    """Run all SAM2 tasks."""
    return eval_sam2_segmentation(run_id=run_id, max_frames=max_frames)


@workflow
def eval_stage_videopainter(
    data_run_id: str = VP_DEFAULT_DATA_RUN_ID,
    output_run_id: Optional[str] = None,
    data_video_ids: str = "auto",
    inpainting_sample_id: int = 0,
    model_path: str = VP_DEFAULT_MODEL_PATH,
    inpainting_branch: str = VP_DEFAULT_BRANCH_PATH,
    img_inpainting_model: str = VP_DEFAULT_IMG_INPAINT_PATH,
    img_inpainting_lora_path: str = VP_DEFAULT_LORA_PATH,
    img_inpainting_lora_scale: float = 0.0,
    output_name_suffix: str = "vp_edit_sample0.mp4",
    num_inference_steps: int = 70,
    guidance_scale: float = 6.0,
    num_videos_per_prompt: int = 1,
    strength: float = 1.0,
    down_sample_fps: int = 8,
    inpainting_frames: int = 49,
    video_editing_instructions: str = "",
    llm_model: str = VP_DEFAULT_LLM_MODEL,
    caption_refine_iters: int = 10,
    caption_refine_temperature: float = 0.1,
    dilate_size: int = 24,
    mask_feather: int = 8,
    keep_masked_pixels: bool = True,
    seed: int = 42,
) -> str:
    """Run all VideoPainter tasks."""
    return eval_videopainter_edit_many(
        data_run_id=data_run_id,
        output_run_id=output_run_id,
        data_video_ids=data_video_ids,
        inpainting_sample_id=inpainting_sample_id,
        model_path=model_path,
        inpainting_branch=inpainting_branch,
        img_inpainting_model=img_inpainting_model,
        img_inpainting_lora_path=img_inpainting_lora_path,
        img_inpainting_lora_scale=img_inpainting_lora_scale,
        output_name_suffix=output_name_suffix,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        num_videos_per_prompt=num_videos_per_prompt,
        strength=strength,
        down_sample_fps=down_sample_fps,
        inpainting_frames=inpainting_frames,
        video_editing_instructions=video_editing_instructions,
        llm_model=llm_model,
        caption_refine_iters=caption_refine_iters,
        caption_refine_temperature=caption_refine_temperature,
        dilate_size=dilate_size,
        mask_feather=mask_feather,
        keep_masked_pixels=keep_masked_pixels,
        seed=seed,
    )


@workflow
def eval_stage_alpamayo(
    video_data_gcs_path: str = f"gs://{BUCKET}/{os.environ.get('ALPAMAYO_VIDEO_DATA_PREFIX', 'workspace/user/hbaskar/outputs/vp')}",
    output_run_id: str = "001",
    model_id: str = ALPAMAYO_DEFAULT_MODEL_ID,
    num_traj_samples: int = 1,
) -> dict:
    """Run all Alpamayo tasks."""
    return eval_alpamayo_inference(
        video_data_gcs_path=video_data_gcs_path,
        output_run_id=output_run_id,
        model_id=model_id,
        num_traj_samples=num_traj_samples,
    )


# ═══════════════════════════════════════════════════════════════════════════
# Full pipeline workflow (all three stages, no data dependency chaining)
# ═══════════════════════════════════════════════════════════════════════════

@workflow
def eval_all(
    # SAM2
    run_id: str = "001",
    max_frames: int = 150,
    # VideoPainter
    vp_data_run_id: str = VP_DEFAULT_DATA_RUN_ID,
    vp_instructions: str = "",
    vp_num_samples: int = 1,
    vp_num_inference_steps: int = 70,
    vp_dilate_size: int = 24,
    vp_mask_feather: int = 8,
    vp_keep_masked_pixels: bool = True,
    vp_llm_model: str = VP_DEFAULT_LLM_MODEL,
    vp_caption_refine_iters: int = 10,
    # Alpamayo
    alpamayo_video_data_gcs_path: str = f"gs://{BUCKET}/{os.environ.get('ALPAMAYO_VIDEO_DATA_PREFIX', 'workspace/user/hbaskar/outputs/vp')}",
    alpamayo_model_id: str = ALPAMAYO_DEFAULT_MODEL_ID,
    alpamayo_num_traj_samples: int = 1,
) -> None:
    """Run all three pipeline stages (SAM2, VideoPainter, Alpamayo)."""
    eval_stage_sam2(run_id=run_id, max_frames=max_frames)
    eval_stage_videopainter(
        data_run_id=vp_data_run_id,
        video_editing_instructions=vp_instructions,
        num_videos_per_prompt=vp_num_samples,
        num_inference_steps=vp_num_inference_steps,
        dilate_size=vp_dilate_size,
        mask_feather=vp_mask_feather,
        keep_masked_pixels=vp_keep_masked_pixels,
        llm_model=vp_llm_model,
        caption_refine_iters=vp_caption_refine_iters,
    )
    eval_stage_alpamayo(
        video_data_gcs_path=alpamayo_video_data_gcs_path,
        output_run_id=run_id,
        model_id=alpamayo_model_id,
        num_traj_samples=alpamayo_num_traj_samples,
    )


# ═══════════════════════════════════════════════════════════════════════════
# Workflow exports
# ═══════════════════════════════════════════════════════════════════════════

WORKFLOWS_BY_CONFIG = {
    "sam2_segmentation": eval_sam2_segmentation,
    "sam2_single_video": eval_sam2_single_video,
    "videopainter_edit_many": eval_videopainter_edit_many,
    "alpamayo_inference": eval_alpamayo_inference,
}

WORKFLOWS_BY_STAGE = {
    "sam2": eval_stage_sam2,
    "videopainter": eval_stage_videopainter,
    "alpamayo": eval_stage_alpamayo,
}

ALL_CONFIGS = list(TASKS_BY_CONFIG.keys())
ALL_STAGES = list(TASKS_BY_STAGE.keys())


# ═══════════════════════════════════════════════════════════════════════════
# Populate __all__
# ═══════════════════════════════════════════════════════════════════════════

__all__ = [
    # Workflows
    "eval_all",
    "eval_sam2_segmentation",
    "eval_sam2_single_video",
    "eval_videopainter_edit_many",
    "eval_alpamayo_inference",
    "eval_stage_sam2",
    "eval_stage_videopainter",
    "eval_stage_alpamayo",
    # Tasks
    "task_sam2_segmentation",
    "task_sam2_single_video",
    "task_videopainter_edit_many",
    "task_alpamayo_inference",
    # Registries and constants
    "TASKS_BY_CONFIG",
    "TASKS_BY_STAGE",
    "WORKFLOWS_BY_CONFIG",
    "WORKFLOWS_BY_STAGE",
    "ALL_CONFIGS",
    "ALL_STAGES",
    "NUM_GPUS",
    "BUCKETS",
    # Container images
    "SAM2_CONTAINER_IMAGE",
    "VP_CONTAINER_IMAGE",
    "ALPAMAYO_CONTAINER_IMAGE",
]
