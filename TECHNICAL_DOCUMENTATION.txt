================================================================================
                    TECHNICAL DOCUMENTATION
              Master Thesis: Video Inpainting for Lane Marking Generation
                     Author: Hariharan Baskar
                     Last Updated: February 17, 2026
================================================================================

TABLE OF CONTENTS
-----------------
1. Project Overview
2. System Architecture
3. Novel Contributions & Modifications
4. Pipeline Components
5. Implementation Details
6. Workflow Orchestration
7. Data Generation & Processing
8. Training Pipeline
9. Evaluation & Results
10. Technical Specifications

================================================================================
1. PROJECT OVERVIEW
================================================================================

This master thesis project implements an end-to-end pipeline for generating and
editing lane markings in autonomous vehicle camera footage using state-of-the-art
video inpainting models, and evaluating the impact of edited lane markings on
autonomous driving trajectory prediction using Vision-Language-Action (VLA) models.

The system combines SAM2 (Segment Anything Model 2) for road segmentation,
VideoPainter (SIGGRAPH 2025) for video inpainting, and Alpamayo-R1-10B for
VLA-based trajectory prediction â€” all orchestrated through a unified three-stage
master pipeline with extensive custom modifications for autonomous driving
applications.

PRIMARY OBJECTIVES:
- Generate synthetic lane marking datasets from Physical AI autonomous vehicle data
- Enable precise control over lane marking attributes (count, color, pattern)
- Train specialized FluxFill LoRA models for lane marking generation
- Evaluate inpainting quality using VLM-based assessment and numerical metrics
- Assess impact of lane marking modifications on autonomous driving trajectory
  prediction using Alpamayo-R1-10B VLA model
- Provide end-to-end pipeline orchestration (SAM2 â†’ VideoPainter â†’ Alpamayo)

KEY TECHNOLOGIES:
- SAM2.1 (Hiera-Large) for video segmentation
- VideoPainter with CogVideoX-5B backbone
- FluxFill image inpainting model
- Qwen2.5-VL Vision Language Model (7B/72B variants)
- Alpamayo-R1-10B Vision-Language-Action model for trajectory prediction
- Google Cloud Platform (GCS) with HLX workflow orchestration
- Master pipeline orchestrator for three-stage sequential execution

================================================================================
2. SYSTEM ARCHITECTURE
================================================================================

The system consists of four major components, orchestrated through a master
pipeline that chains three GPU stages sequentially:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     COMPONENT 1: DATA GENERATION                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Physical AIâ”‚->â”‚ SAM2     â”‚->â”‚  Qwen2.5 â”‚->â”‚ FluxFill Dataset â”‚  â”‚
â”‚  â”‚ Videos     â”‚  â”‚Segmenta- â”‚  â”‚  Caption â”‚  â”‚  (CSV + Images)  â”‚  â”‚
â”‚  â”‚  (mp4)     â”‚  â”‚  tion    â”‚  â”‚ Generate â”‚  â”‚                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   COMPONENT 2: DATA FILTERING/SORTING                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚FluxFill      â”‚->â”‚ Filter by Lane   â”‚->â”‚ Sorted & Filtered    â”‚  â”‚
â”‚  â”‚Dataset (Raw) â”‚  â”‚ Attributes       â”‚  â”‚ Training Dataset     â”‚  â”‚
â”‚  â”‚              â”‚  â”‚ (count/color/    â”‚  â”‚                      â”‚  â”‚
â”‚  â”‚              â”‚  â”‚  pattern)        â”‚  â”‚                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPONENT 3: TRAINING & INFERENCE                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Filtered      â”‚->â”‚ FluxFill LoRA    â”‚->â”‚ LoRA Checkpoints     â”‚  â”‚
â”‚  â”‚Dataset       â”‚  â”‚ Training         â”‚  â”‚                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚LoRA Ckpts +  â”‚->â”‚ VideoPainter     â”‚->â”‚ Generated/Edited     â”‚  â”‚
â”‚  â”‚Input Videos  â”‚  â”‚ Edit Pipeline    â”‚  â”‚ Videos               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           COMPONENT 4: VLA TRAJECTORY EVALUATION (NEW)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Edited       â”‚->â”‚ Alpamayo-R1-10B  â”‚->â”‚ Trajectory Preds +   â”‚  â”‚
â”‚  â”‚ Videos       â”‚  â”‚ VLA Inference    â”‚  â”‚ Reasoning Traces +   â”‚  â”‚
â”‚  â”‚              â”‚  â”‚ + PhysicalAI AV  â”‚  â”‚ minADE Metrics +     â”‚  â”‚
â”‚  â”‚              â”‚  â”‚   ego-motion     â”‚  â”‚ Overlay Videos       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MASTER PIPELINE ORCHESTRATION (workflow_master.py)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
All three inference stages are chained in a single HLX workflow with
a shared run_id for end-to-end traceability:

  SAM2 â”€â”€(run_id)â”€â”€â–¶ VideoPainter â”€â”€(gcs_path)â”€â”€â–¶ Alpamayo

  =========  ====================================================
  Stage      GCS output
  =========  ====================================================
  SAM2       gs://â€¦/outputs/sam2/<run_id>/
  SAM2 (VP)  gs://â€¦/outputs/preprocessed_data_vp/<run_id>/
  VP         gs://â€¦/outputs/vp/<run_id>/
  Alpamayo   gs://â€¦/outputs/alpamayo/<run_id>/
  =========  ====================================================

Each stage runs in its own container image (heavy ML deps are isolated)
while the workflow graph is serialised from a lightweight orchestrator
container. Data-dependency edges enforce strict sequential execution.

================================================================================
3. NOVEL CONTRIBUTIONS & MODIFICATIONS
================================================================================

ðŸŒŸ MAJOR CONTRIBUTION #1: FLUXFILL INTEGRATION IN EDIT_BENCH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ORIGINAL: VideoPainter's edit_bench.py used only CogVideoX-based models
MODIFIED: Integrated FluxFill image inpainting model as first-frame generator

FILE: generation/VideoPainter/infer/edit_bench.py

KEY MODIFICATIONS:
1. Added FluxFillPipeline import and initialization
2. Implemented dual-model pipeline: FluxFill â†’ CogVideoX
   - FluxFill generates high-quality first frame
   - CogVideoX propagates edits to subsequent frames
3. Created VLM-based iterative refinement loop:
   - Qwen2.5-VL evaluates FluxFill first-frame quality
   - Auto-refines captions until quality threshold met
   - Provides structured feedback (PASS/FAIL with reasoning)
4. Implemented intelligent mask processing:
   - Dilation and feathering for seamless inpainting
   - Background context preservation
5. Multi-GPU orchestration:
   - Qwen VLM on cuda:1 (if available)
   - FluxFill + CogVideoX on cuda:0
   - Automatic VRAM management and model offloading

TECHNICAL DETAILS:
- Function: _qwen_refine_first_frame_caption()
  * Evaluates inpainting against instruction semantics
  * Validates lane marking attributes (single/double, color, pattern)
  * Generates revised captions with visual texture details
  * Configurable maximum refinement iterations (default: 10 in deployment)

- Function: _dilate_and_feather_mask_images()
  * Expands inpaint regions using morphological dilation
  * Applies Gaussian blur for smooth boundaries
  * Configurable kernel sizes for different mask types

- Multi-model coordination:
  * Sequential execution: Qwen â†’ FluxFill â†’ CogVideoX
  * Explicit model unloading between stages (_unload_qwen_model)
  * GPU memory tracking and optimization

IMPACT: Improved first-frame semantic correctness and visual quality
        (quantitative results pending systematic evaluation)


ðŸŒŸ MAJOR CONTRIBUTION #2: SAM2 ROAD SEGMENTATION PIPELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Created complete SAM2-based road segmentation system for autonomous driving videos

FILES:
- segmentation/sam2/process_videos_sam2.py (main processing logic)
- segmentation/sam2/workflow.py (HLX workflow orchestration)
- segmentation/sam2/process_vide_sam2_hlxwf.py (workflow wrapper)

KEY FEATURES:
1. Automatic road region detection using fixed point grid:
   - 9-point grid positioned for typical road perspective
   - Points at (0.5, 0.85), (0.4, 0.75), (0.6, 0.75), etc.
   - Optimized for front-facing vehicle cameras
   
2. SAM2VideoPredictorVOS integration:
   - Uses sam2.1_hiera_large.pt checkpoint (best quality)
   - Video object segmentation (VOS) mode for temporal consistency
   - Frame-by-frame mask propagation
   
3. Multi-format output generation:
   - Binary masks (.png) for each frame
   - Compressed NPZ archives for efficient storage
   - Visualization overlays (segmented videos)
   - Metadata CSV with video statistics
   
4. VideoPainter-compatible preprocessing:
   - Creates proper folder structure (raw_videos/, masks/, meta.csv)
   - Handles FPS normalization (8 FPS default for training)
   - Automatic video transcoding with ffmpeg
   
5. GCS integration:
   - Direct upload/download from Google Cloud Storage
   - Parallel processing of multiple videos
   - Resumable operations with caching

TECHNICAL SPECIFICATIONS:
- Input: MP4 videos from Physical AI dataset
- Processing: 150 frames per video (configurable)
- Mask format: Binary PNG (0=keep, 255=inpaint)
- FPS: 8 fps for training data, source FPS detected automatically
- Checkpoint: sam2.1_hiera_large.pt (305M parameters)

INNOVATION: First automated road segmentation pipeline specifically designed
            for video inpainting training data generation


ðŸŒŸ MAJOR CONTRIBUTION #3: AUTOMATED DATASET GENERATION FROM PHYSICAL AI
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Implemented complete automated pipeline to generate FluxFill training datasets
from Physical AI autonomous vehicle videos

FILE: segmentation/sam2/data_generation.py

PIPELINE STAGES:
1. GCS Video Discovery:
   - Chunk-based folder structure parsing (chunk_0000 to chunk_0199)
   - Configurable video slice selection (start_index + limit)
   - Smart filtering to avoid duplicate processing
   
2. Frame Extraction:
   - Multi-frame extraction per video (configurable frame numbers)
   - ffmpeg-based precise frame selection
   - Automatic quality validation
   
3. SAM2 Mask Generation:
   - Road region segmentation for each frame
   - Binary mask creation (white=road to inpaint)
   - Quality filtering (minimum mask coverage)
   
4. VLM Caption Generation (Qwen2.5-VL):
   - Automatic scene understanding
   - Lane marking attribute detection:
     * Count: single/double/unknown
     * Color: white/yellow/mixed/unknown
     * Pattern: solid/dashed/mixed/unknown
   - Structured prompt format:
     "road with {count} {color} {pattern} lane markings"
   
5. FluxFill Dataset Assembly:
   - CSV creation: image, mask, prompt, prompt_2
   - Relative path structure for portability
   - GCS upload with proper folder hierarchy

OUTPUT FORMAT:
  dataset_folder/
    â”œâ”€â”€ train.csv
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ video1_frame001.png
    â”‚   â”œâ”€â”€ video1_frame100.png
    â”‚   â””â”€â”€ ...
    â””â”€â”€ masks/
        â”œâ”€â”€ video1_frame001.png
        â”œâ”€â”€ video1_frame100.png
        â””â”€â”€ ...

DATASET SCALE: Successfully generated 10,000+ training samples from Physical AI

INNOVATION: First fully automated lane marking dataset generation pipeline
            combining SAM2 + VLM for semantic attribute extraction


ðŸŒŸ MAJOR CONTRIBUTION #4: INTELLIGENT DATA SORTING & FILTERING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Created advanced filtering system to curate training datasets by lane attributes

FILE: segmentation/sam2/filter_fluxfill_dataset.py

FILTERING CAPABILITIES:
1. Attribute-based selection:
   - Count filtering: single/double/any
   - Color filtering: white/yellow/mixed/any
   - Pattern filtering: solid/dashed/mixed/any
   - Combination support: "single white solid" dataset
   
2. Quality control:
   - REQUIRE_CLEAR_ROAD flag to exclude "unknown" attributes
   - Regex-based prompt validation
   - Automatic rejection of ambiguous samples
   
3. Dataset operations:
   - Copy/hardlink/symlink modes (configurable)
   - Automatic output naming with suffix
   - CSV reordering (by prompt or image name)
   - Optional sample limiting
   
4. GCS support:
   - Direct GCS input/output
   - Local staging for efficiency
   - Batch upload after filtering

REGEX PARSER:
  Pattern: r"road with (?P<count>...) (?P<color>...) (?P<pattern>...) lane markings"
  Extracts: count, color, pattern attributes
  Validates: against allowed value sets

WORKFLOW INTEGRATION:
  Script: segmentation/sam2/scripts/build_and_run_sorting_data.sh
  Configures: COUNT, COLOR, PATTERN, REQUIRE_CLEAR_ROAD
  Output: Automatically named: {input}__{suffix}

EXAMPLE USE CASES:
  - Create "single white solid" training set (10,000 samples)
  - Extract all "yellow dashed" markings
  - Build mixed-attribute datasets for generalization

INNOVATION: First semantic lane marking dataset curation tool with multi-
            attribute filtering and quality validation


ðŸŒŸ MAJOR CONTRIBUTION #5: FLUXFILL LORA TRAINING PIPELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Implemented complete LoRA fine-tuning pipeline for FluxFill image inpainting

FILES:
- generation/VideoPainter/train/train_fluxfill_inpaint_lora.py
- generation/VideoPainter/training_workflow.py

TRAINING SYSTEM:
1. Custom dataset loader:
   - CSV-based data loading (image, mask, prompt, prompt_2)
   - Relative path resolution
   - Multi-prompt support (primary + secondary)
   
2. LoRA configuration:
   - Target modules: transformer blocks
   - Rank: 128 (configurable)
   - Alpha: 128
   - Dropout: 0.0
   
3. Training optimizations:
   - Mixed precision (BF16)
   - Gradient accumulation
   - AdamW optimizer with cosine LR schedule
   - Checkpointing every N steps
   
4. HLX workflow integration:
   - FuseBucket mounts for checkpoints
   - GCS dataset mounting
   - Automatic output uploading
   - A100 80GB GPU compute node

HYPERPARAMETERS (configurable):
  - max_train_steps: Empty (trains on full dataset)
  - num_train_epochs: 5 (default)
  - learning_rate: 1e-5 (reduced for stability)
  - lr_scheduler: "cosine"
  - lr_warmup_steps: 50
  - train_batch_size: 1
  - gradient_accumulation_steps: 4 (effective batch size: 4)
  - checkpointing_steps: 100
  - mixed_precision: "bf16"

WORKFLOW FEATURES:
  - Symlink-based checkpoint mounting (VRAM efficient)
  - Multi-GPU support
  - Automatic run_id generation
  - GCS output directory creation

TRAINING SCRIPT: scripts/build_and_run_training.sh
  - Configures training parameters
  - Builds Docker container
  - Pushes to GCR
  - Launches HLX workflow
  - Auto-generates timestamp-based run IDs
  - Default training: 5 epochs on full dataset
  - Configurable via environment variables

OUTPUT: LoRA checkpoints compatible with FluxFillPipeline

INNOVATION: First LoRA training pipeline specifically for FluxFill inpainting
            on structured lane marking datasets


ðŸŒŸ CONTRIBUTION #6: ALPAMAYO VLA TRAJECTORY EVALUATION PIPELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Integrated Alpamayo-R1-10B Vision-Language-Action model as the third stage
of the pipeline to evaluate how edited lane markings affect autonomous
driving trajectory prediction.

FILES:
- vla/alpamayo/run_inference.py (469 lines â€” core inference logic)
- vla/alpamayo/workflow_alpamayo.py (550 lines â€” HLX workflow)
- vla/alpamayo/visualize_video.py (530 lines â€” trajectory overlay rendering)
- vla/alpamayo/download_checkpoints.py (checkpoint download utility)
- vla/alpamayo/scripts/build_and_run.sh (standalone build/run script)
- vla/alpamayo/Dockerfile (Python 3.12, CUDA 12.1, flash-attn)
- vla/alpamayo/src/alpamayo_r1/ (model source: models, geometry, diffusion)

KEY FEATURES:
1. VideoPainter-to-Alpamayo data bridge:
   - Parses clip_id and camera_name from VP output filenames
     (pattern: <clip_id>.<camera_name>_vp_edit_sample0.mp4)
   - Loads original driving clip from NVIDIA PhysicalAI-AV dataset
     via HuggingFace streaming (requires HF_TOKEN)
   - Replaces the inpainted camera's frames with VP-decoded frames
   - Maintains temporal alignment: VP fps â†’ dataset time_step matching

2. Multi-camera trajectory inference:
   - 7 camera views supported (front_tele, front_wide, cross_left/right,
     rear_left/right, rear_tele)
   - Uses last 4 consecutive frames per camera at VP video fps
   - Ego-motion history (16 timesteps) from PhysicalAI-AV dataset
   - Chain-of-thought reasoning traces from VLA model

3. Quantitative evaluation:
   - minADE (minimum Average Displacement Error) vs ground truth
     Formula: min_s (1/T) Î£_t ||pred_xy^(s)_t âˆ’ gt_xy_t||_2 over XY only
   - Per-video metrics: inference time, GPU memory (current+peak), RAM usage
   - Aggregate reports with avg/best/worst minADE across all videos

4. Visualization outputs:
   - Trajectory overlay videos (H.264/MP4, CRF=18, yuv420p)
   - Pinhole camera projection: egoâ†’camera transform
     (x_cam = âˆ’y_ego, y_cam = âˆ’z_ego, z_cam = x_ego)
   - Focal length: f_x = (W/2) / tan(FOV/2)
   - Ground-truth trajectory drawn in red, predictions in cyan/green/etc.
   - Bird's-eye-view (BEV) inset (250Ã—250 px) at bottom-left corner
     with ego history (gray), GT (red), and predictions (cyan)
   - Progressive reveal mode: linearly maps frame index â†’ waypoint count
   - Points behind camera (z_cam â‰¤ 0.5) are filtered out
   - FOV auto-detected from camera name (120Â°, 70Â°, or 30Â°)
   - 5 cycling colors for multiple trajectory samples
   - Semi-transparent legend box in top-right corner
   - HUD text overlay with minADE and clip/camera info from JSON
   - NPZ visualization data for offline plotting

5. Auto-discovery of VP output videos:
   - Scans FuseBucket-mounted VP output directory
   - Filters out _generated.mp4 sidecar files
   - Optional video_name filter for single-video processing

OUTPUT FORMAT:
  alpamayo_output/<run_id>/
    â”œâ”€â”€ <video_stem>/
    â”‚   â”œâ”€â”€ <video_stem>_inference.json   (predictions + reasoning + minADE)
    â”‚   â”œâ”€â”€ <video_stem>_vis_data.npz     (visualization tensors)
    â”‚   â””â”€â”€ <video_stem>_overlay.mp4      (trajectory overlay video)
    â””â”€â”€ <run_id>_report.txt               (aggregate metrics report)

INFERENCE JSON FORMAT:
  {
    "video_id", "video_path", "clip_id", "camera_name",
    "num_trajectories", "min_ade_meters",
    "reasoning_traces": ["<chain-of-thought text>"],
    "temporal_config": {
      "vp_video_fps", "time_step_seconds",
      "num_frames_per_camera", "frame_window_seconds"
    },
    "metrics": {
      "inference_time_seconds", "gpu_memory_used_gb",
      "gpu_memory_peak_gb", "ram_used_mb", "ram_peak_mb"
    },
    "success": true
  }

VISUALIZATION NPZ FORMAT:
  pred_xyz:        (S, 64, 3)     â€” predicted trajectories
  pred_rot:        (S, 64, 3, 3)  â€” predicted rotations
  gt_future_xyz:   (64, 3)        â€” GT future trajectory
  gt_future_rot:   (64, 3, 3)     â€” GT future rotations
  ego_history_xyz: (16, 3)        â€” ego history positions
  ego_history_rot: (16, 3, 3)     â€” ego history rotations
  image_frames:    (N_camÃ—T, H, W, 3) â€” all camera frames uint8
  camera_indices:  (N_cam,)       â€” camera index array

COMPUTE:
  - Model: Alpamayo-R1-10B (bfloat16)
  - GPU: A100 80GB (1 GPU)
  - Inference time recorded per-video in output JSON
  - Python 3.12 + CUDA 12.1 + flash-attention
  - Inference params: top_p=0.98, temperature=0.6, max_generation_length=256

INNOVATION: First integration of VLA trajectory prediction as an evaluation
            metric for video inpainting quality in autonomous driving


ðŸŒŸ CONTRIBUTION #7: MASTER PIPELINE ORCHESTRATOR
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Created a unified three-stage master workflow that chains SAM2 â†’ VideoPainter
â†’ Alpamayo in a single HLX invocation with a shared run_id.

FILES:
- workflow_master.py (427 lines â€” master workflow definition)
- scripts/build_and_run.sh (300 lines â€” master build/run script)
- Dockerfile (54 lines â€” lightweight orchestrator container)
- docker-compose.yaml (100 lines â€” master + optional local services)

KEY FEATURES:
1. Three-stage sequential execution:
   - Stage 1 (sam2_stage): SAM2 segmentation in SAM2 container
   - Stage 2 (vp_stage): VideoPainter editing in VP container
   - Stage 3 (alpamayo_stage): VLA inference in Alpamayo container
   - Data-dependency edges: SAM2â†’VP via run_id, VPâ†’Alpamayo via gcs_path

2. Container isolation:
   - Each stage runs in its own specialized container image
   - Master orchestrator is a lightweight Python 3.10-slim container
   - No heavy ML dependencies in orchestrator (only gcsfs, hlx-wf)
   - Each stage imports its own workflow module at runtime

3. Unified build_and_run.sh:
   - Builds all 4 Docker images (SAM2, VP, Alpamayo, Master)
   - Tags with shared MASTER_RUN_ID (RUN_ID + timestamp)
   - Pushes all images to Artifact Registry
   - Submits master workflow with all parameters
   - SKIP_BUILD=1 mode to reuse previously pushed images

4. Multi-instruction VideoPainter support:
   - 5 default lane instructions (single/double Ã— white/yellow Ã— solid/dashed)
   - Each instruction generates its own output subfolder
   - All instructions processed per video before moving to next video

5. chunks:// URI scheme for SAM2 input:
   - Format: chunks://<bucket>/<prefix>?start=N&end=M&per_chunk=K
   - Resolves chunk_NNNN/ folders dynamically via gcsfs
   - Configurable chunk range and files per chunk

6. Comprehensive parameter forwarding:
   - SAM2: video URIs, max frames
   - VP: inference steps, guidance scale, strength, refine iters, temperature,
     dilate size, mask feather, LoRA scale, seed, instructions
   - Alpamayo: model ID, trajectory samples, video name filter
   - All configurable via environment variables

MASTER WORKFLOW PARAMETERS (workflow_master.master_pipeline_wf):
  run_id                          Shared ID across all stages
  sam2_video_uris                 chunks:// or gs:// input
  sam2_max_frames                 Max frames per video (default: 150)
  vp_video_editing_instructions   Instructions (|| separated or newlines)
  vp_llm_model                    Qwen VLM path
  vp_num_inference_steps          CogVideoX steps (default: 70)
  vp_guidance_scale               Classifier-free guidance (default: 6.0)
  vp_strength                     Inpainting strength (default: 1.0)
  vp_caption_refine_iters         VLM refinement iterations (default: 10)
  vp_caption_refine_temperature   VLM temperature (default: 0.1)
  vp_dilate_size                  Mask dilation (default: 24)
  vp_mask_feather                 Mask feathering (default: 8)
  vp_keep_masked_pixels           Preserve original masked pixels
  vp_img_inpainting_lora_scale    LoRA scale (default: 0.0)
  vp_seed                         Random seed (default: 42)
  alp_model_id                    Alpamayo model path
  alp_num_traj_samples            Trajectory samples (default: 1)
  alp_video_name                  Video filter ("auto" = all)

INNOVATION: First end-to-end orchestration combining video segmentation,
            inpainting, and VLA trajectory evaluation in a single workflow


ðŸŒŸ CONTRIBUTION #8: VIDEO QUALITY EVALUATION METRICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Integrated comprehensive numerical video quality metrics into the
VideoPainter workflow for automated evaluation of inpainting results.

LOCATION: generation/VideoPainter/workflow_vp.py (_evaluate_video function)

METRICS COMPUTED:
1. Full-frame metrics:
   - PSNR (Peak Signal-to-Noise Ratio)
   - SSIM (Structural Similarity Index)
   - LPIPS (Learned Perceptual Image Patch Similarity)
   - MSE (Mean Squared Error)
   - MAE (Mean Absolute Error)
   - Temporal Consistency (frame-to-frame stability)
   - CLIP Score (text-video alignment, when caption available)

2. Masked-region metrics (same metrics computed only in inpainted area):
   - All of the above, computed only where mask > 0.5
   - Enables measuring inpainting quality independent of background

3. Quality assessment classification:
   - EXCELLENT: PSNR > 40 + SSIM > 0.95 + LPIPS < 0.1
   - GOOD: One of the above thresholds met
   - MODERATE: Below thresholds

4. Automatic upload: eval_{video_id}.txt saved alongside generated video in GCS

INNOVATION: First automated per-video quality evaluation integrated directly
            into the video inpainting workflow pipeline


ðŸŒŸ CONTRIBUTION #9: END-TO-END WORKFLOW ORCHESTRATION (UPDATED)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Expanded the HLX workflow system from 5 to 8 workflows with the addition of
the master orchestrator, Alpamayo VLA, and renamed/restructured workflow files.

WORKFLOW FILES (UPDATED):
1. workflow_master.py (NEW â€” master orchestrator)
   - master_pipeline_wf: SAM2 â†’ VP â†’ Alpamayo
   - sam2_stage, vp_stage, alpamayo_stage tasks
   - Shared run_id across all stages
   
2. segmentation/sam2/workflow_sam2.py (RENAMED from workflow.py)
   - sam2_segmentation_wf: SAM2 video segmentation workflow
   - run_sam2_segmentation: core task
   - chunks:// URI resolution via _resolve_chunk_uri()
   - GCS video download with caching
   
3. segmentation/sam2/data_generation.py (workflow)
   - FluxFill dataset generation workflow
   - VLM integration for captioning
   - Chunk-based video discovery
   
4. segmentation/sam2/filter_fluxfill_dataset.py (workflow)
   - Dataset filtering workflow
   - Attribute-based selection
   - Quality control
   
5. generation/VideoPainter/training_workflow.py
   - FluxFill LoRA training workflow
   - Checkpoint management
   - Multi-stage training support
   
6. generation/VideoPainter/workflow_vp.py (RENAMED from workflow.py)
   - videopainter_many_wf: multi-video, multi-instruction editing
   - run_videopainter_edit_many: core task (1401 lines)
   - Multi-instruction support (|| delimited or newline-separated)
   - Per-video numerical evaluation (PSNR/SSIM/LPIPS/CLIP)
   - VPVideoMetrics dataclass for per-video performance tracking
   - Automatic sidecar artifact upload (JSON, debug images, refine logs)
   
7. vla/alpamayo/workflow_alpamayo.py (NEW)
   - alpamayo_vla_inference_wf: VLA trajectory inference
   - run_alpamayo_inference_task: core task
   - VLAVideoMetrics dataclass with minADE tracking
   - Comprehensive report generation with aggregate statistics
   - FuseBucket-based video data and checkpoint mounting

BUILD SCRIPTS (UPDATED):
  scripts/build_and_run.sh                  NEW â€” Master pipeline (builds all 4 images)
  segmentation/sam2/scripts/build_and_run.sh
  segmentation/sam2/scripts/build_and_run_training_data.sh
  segmentation/sam2/scripts/build_and_run_sorting_data.sh
  generation/VideoPainter/scripts/build_and_run.sh
  generation/VideoPainter/scripts/build_and_run_training.sh
  generation/VideoPainter/scripts/download_physicalai_data.sh  NEW
  vla/alpamayo/scripts/build_and_run.sh     NEW â€” Standalone Alpamayo

COMPUTE RESOURCES:
  - A100 80GB GPUs (1 per task, configurable)
  - 3-day maximum duration per task
  - Maximum ephemeral storage
  - Dedicated nodes for each stage

DOCKER CONTAINERS (UPDATED):
  1. Master Orchestrator: python:3.10-slim (lightweight, no GPU)
  2. SAM2: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
  3. VideoPainter: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
  4. Alpamayo: nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 (Python 3.12)

INNOVATION: Complete cloud-native workflow orchestration for multi-stage
            video inpainting and trajectory evaluation pipeline with
            automatic resource management and end-to-end traceability

================================================================================
4. PIPELINE COMPONENTS DETAIL
================================================================================

COMPONENT A: VIDEO PREPROCESSING (SAM2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Purpose: Extract road regions from autonomous vehicle videos
Input:   Physical AI dataset videos (MP4 format, FPS auto-detected per video)
Output:  Segmented videos + masks + VideoPainter-compatible structure

Process Flow:
1. Video download from GCS (with caching)
2. Frame extraction at specified FPS
3. SAM2.1 Hiera-Large video predictor initialization
4. Point-grid based road segmentation
5. Temporal mask propagation across frames
6. Mask post-processing (dilation, filtering)
7. Video reconstruction with segmented regions
8. Output packaging for VideoPainter

Key Parameters:
- SAM2_CHECKPOINT: sam2.1_hiera_large.pt
- MODEL_CFG: configs/sam2.1/sam2.1_hiera_l.yaml
- MAX_FRAMES: 150
- FRAMES_PER_SECOND: 8
- Point grid: 9 points at road-optimized positions

Output Structure:
  preprocessed_data_vp/{run_id}/{video_id}/
    â”œâ”€â”€ meta.csv                  (fps, frames, dimensions)
    â”œâ”€â”€ raw_videos/
    â”‚   â””â”€â”€ {vid}/
    â”‚       â””â”€â”€ {video_id}.0.mp4  (transcoded to 8fps)
    â””â”€â”€ mask_root/
        â””â”€â”€ {video_id}/
            â””â”€â”€ all_masks.npz      (compressed binary masks)


COMPONENT B: DATASET GENERATION (SAM2 + QWEN)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Purpose: Create FluxFill training datasets with semantic lane attributes
Input:   Physical AI video chunks (organized in GCS folders)
Output:  CSV + images + masks for FluxFill training

Process Flow:
1. GCS prefix traversal with chunk range selection
2. Video sampling (configurable start/limit)
3. Multi-frame extraction per video
4. SAM2 mask generation for each frame
5. Qwen2.5-VL caption generation with lane attribute detection
6. Prompt normalization to structured format
7. CSV assembly with relative paths
8. Dataset upload to GCS

Caption Generation Details:
  System Prompt:
    "You are analyzing road images with masked regions. Describe the 
     lane markings visible, including count (single/double), color 
     (white/yellow/mixed), and pattern (solid/dashed/mixed)."
  
  Output Format:
    "road with {count} {color} {pattern} lane markings"
  
  Examples:
    - "road with single white solid lane markings"
    - "road with double yellow dashed lane markings"
    - "road with single white mixed lane markings"

Quality Filters:
  - Minimum mask coverage: 1% of frame
  - Valid caption structure required
  - Frame extraction validation

Scale: 10,000+ samples generated from 200 video chunks


COMPONENT C: DATASET FILTERING & SORTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Purpose: Curate training datasets by specific lane attributes
Input:   Raw FluxFill dataset from Component B
Output:  Filtered dataset matching specified criteria

Filtering Logic:
  For each row in train.csv:
    1. Parse prompt with regex to extract attributes
    2. Check count filter (if not "any")
    3. Check color filter (if not "any")
    4. Check pattern filter (if not "any")
    5. If REQUIRE_CLEAR_ROAD=1, reject "unknown" attributes
    6. If all filters pass, copy image + mask to output

Supported Filters:
  Count:   single | double | unknown | any
  Color:   white | yellow | mixed | unknown | any
  Pattern: solid | dashed | mixed | unknown | any

Output Naming:
  Automatic suffix generation based on filters
  Example: original_dataset__single_white_solid

CSV Operations:
  - Sorting by prompt (groups similar samples)
  - Sorting by image name (chronological)
  - Optional row limit for quick experiments

GCS Optimization:
  - Local staging directory for filtering
  - Batch upload after completion
  - Hardlink/symlink support for local processing


COMPONENT D: FLUXFILL LORA TRAINING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Purpose: Fine-tune FluxFill for domain-specific lane marking generation
Input:   Filtered FluxFill dataset (CSV + images + masks)
Output:  LoRA checkpoint weights

Training Architecture:
  Base Model: FLUX.1 Fill (black-forest-labs)
  LoRA Config:
    - Target: transformer blocks
    - Rank: 128
    - Alpha: 128
    - Modules: ["to_k", "to_q", "to_v", "to_out.0"]
  
Training Loop:
  For each batch:
    1. Load image, mask, prompts from CSV
    2. Preprocess images to model input size
    3. Encode with VAE
    4. Add noise (diffusion forward process)
    5. Predict noise with transformer + LoRA
    6. Compute MSE loss
    7. Backward pass with gradient accumulation
    8. Optimizer step (AdamW)
    9. LR scheduler step (cosine annealing)
    10. Checkpoint saving at intervals

Accelerate Integration:
  - Multi-GPU support (DistributedDataParallel)
  - Mixed precision training (BF16)
  - Gradient accumulation
  - Automatic device placement

Checkpointing:
  Saves every N steps:
    - LoRA adapter weights
    - Optimizer state
    - LR scheduler state
    - Training metadata

Typical Training Run:
  Dataset: 10,000 samples (single white solid)
  Steps: depends on dataset size and num_train_epochs
  Time: recorded in training_summary.txt output
  Checkpoints: saved every checkpointing_steps intervals
  Final size: LoRA adapter weights only (much smaller than base model)


COMPONENT E: VIDEO EDITING INFERENCE (VIDEOPAINTER + FLUXFILL)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Purpose: Generate edited videos with new/modified lane markings
Input:   SAM2 preprocessed videos + editing instructions
Output:  Edited videos with seamlessly modified lane markings

Pipeline Stages:

STAGE 1: First Frame Generation (FluxFill + VLM Refinement)
  1. Load first frame + mask
  2. Generate initial caption from instruction
  3. Run FluxFill inpainting
  4. VLM evaluation (Qwen2.5-VL):
     - Create 3-panel comparison image
     - Assess semantic correctness
     - Check visual consistency
     - Return PASS/FAIL + revised caption
  5. If FAIL: refine caption and retry (max 5 iterations)
  6. If PASS: proceed to video generation
  
  VLM Evaluation Criteria:
    âœ“ Semantic match (single/double, color, pattern)
    âœ“ Visual texture (paint wear, crispness)
    âœ“ Perspective alignment
    âœ“ Lighting/shadow consistency
    âœ“ Background preservation

STAGE 2: Video Propagation (CogVideoX)
  1. Load CogVideoX-5B-I2V + VideoPainter branch model
  2. Use first frame as conditioning
  3. Process video chunks (9 frames per chunk)
  4. Apply target region ID resampling for long videos
  5. Blend chunks with temporal smoothing
  6. Export final edited video

Multi-GPU Strategy:
  - Qwen VLM: cuda:1 (if 2 GPUs available), or auto with unload
  - FluxFill: cuda:0
  - CogVideoX: cuda:0
  - Sequential execution with explicit model unloading
  - VP_UNLOAD_QWEN_AFTER_USE=1 for single-GPU memory management

Multi-Instruction Support (NEW):
  - Multiple editing instructions per video (|| delimited or newline-separated)
  - Each instruction gets its own output subfolder
  - Default 5 instructions:
    1. Single solid white continuous line
    2. Double solid white continuous line
    3. Single solid yellow continuous line
    4. Double solid yellow continuous line
    5. Single dashed white intermitted line
  - All instructions include: "aligned exactly to the original lane positions
    and perspective; keep road texture, lighting, and shadows unchanged"

Numerical Evaluation (NEW):
  - Per-video evaluation automatically after generation via _evaluate_video()
  - Uses MetricsCalculator (from evaluate/metrics.py) on GPU tensors [T,C,H,W]
  - Full-frame metrics: PSNR, SSIM, LPIPS, MSE, MAE, Temporal Consistency
  - Masked-region metrics: Same metrics computed only where mask > 0.5
  - CLIP Score: Text-video alignment using instruction as caption
  - Quality assessment: EXCELLENT (â‰¥2 thresholds) / GOOD (1) / MODERATE (0)
    Thresholds: PSNR>40dB, SSIM>0.95, LPIPS<0.1
  - Results saved as eval_{video_id}.txt alongside generated video
  - Output includes: video metadata, resolution, FPS, mask coverage %,
    full-frame metrics block, masked-region metrics block, quality label
  - Quality classification: EXCELLENT / GOOD / MODERATE

Performance:
  - Timing depends on VLM refinement iterations and video length
  - Per-video generation time is tracked in VPVideoMetrics.generation_s
  - First-frame inpainting time depends on number of VLM refinement iterations


COMPONENT F: VLA TRAJECTORY EVALUATION (NEW â€” ALPAMAYO)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Purpose: Evaluate impact of edited lane markings on autonomous driving
         trajectory prediction using Vision-Language-Action models
Input:   VideoPainter-edited videos + PhysicalAI-AV driving data
Output:  Trajectory predictions, minADE metrics, reasoning traces,
         overlay visualization videos

Pipeline Stages:

STAGE 1: Video-to-Dataset Bridge
  1. Parse clip_id and camera_name from VP output filename
     Pattern: <clip_id>.<camera_name>_vp_edit_sample0.mp4
  2. Decode last 4 consecutive frames from VP video using PyAV
  3. Detect VP video fps and compute matching time_step
     (e.g., 8 fps â†’ 0.125s time_step)
  4. Load original clip from NVIDIA PhysicalAI-AV HuggingFace dataset
     using matched time_step for temporal alignment
  5. Replace inpainted camera's frames with VP-decoded frames
     (resize if dimensions differ via bilinear interpolation)

STAGE 2: VLA Inference
  1. Build multi-camera input: 7 cameras Ã— 4 frames = 28 images
  2. Create chat-template messages via Alpamayo helper
  3. Provide ego_history_xyz and ego_history_rot (16 timesteps)
  4. Run model.sample_trajectories_from_data_with_vlm_rollout()
     with top_p=0.98, temperature=0.6
  5. Extract predicted trajectories (S Ã— 64 Ã— 3) and rotations
  6. Extract chain-of-thought reasoning traces

STAGE 3: Evaluation & Visualization
  1. Compute minADE: min over S samples of mean L2 displacement
     between predicted and ground-truth future XY trajectory
     Formula: minADE = min_s (1/T) Î£_t ||pred_xy^(s)_t âˆ’ gt_xy_t||_2
  2. Save inference JSON with predictions, reasoning, temporal config,
     and per-video compute metrics (time, GPU mem, RAM)
  3. Save visualization NPZ with all trajectory tensors + camera frames
     for offline plotting (pred_xyz, gt_future_xyz, ego_history_xyz,
     pred_rot, gt_future_rot, ego_history_rot, image_frames)
  4. Render overlay video (H.264/MP4, CRF=18, yuv420p):
     - Pinhole projection: egoâ†’camera (x_cam=âˆ’y_ego, y_cam=âˆ’z_ego,
       z_cam=x_ego), focal length f_x = (W/2)/tan(FOV/2)
     - Ground-truth trajectory in red (thickness=3, radius=5)
     - Predicted trajectories in 5 cycling colors (cyan, green,
       orange, yellow, magenta) with thickness=2, radius=4
     - Ego history path in gray (thickness=1, radius=3)
     - Bird's-eye-view (BEV) inset (250Ã—250 px) at bottom-left:
       display frame dx=âˆ’y, dy=x (forward=up), auto-scaled
     - Progressive reveal: frame index linearly mapped to waypoint count
     - FOV auto-detected from camera name (120Â°, 70Â°, 30Â°)
     - Semi-transparent legend box (top-right) with color key
     - HUD text: minADE value, clip_id, camera_name from JSON
     - Points behind camera (z_cam â‰¤ 0.5) filtered out

Camera Index Map:
  camera_cross_left_120fov:  0
  camera_front_wide_120fov:  1
  camera_cross_right_120fov: 2
  camera_rear_left_70fov:    3
  camera_rear_tele_30fov:    4
  camera_rear_right_70fov:   5
  camera_front_tele_30fov:   6

Output Structure:
  alpamayo_output/<run_id>/
    â”œâ”€â”€ <video_stem>/
    â”‚   â”œâ”€â”€ <video_stem>_inference.json   (predictions + reasoning + minADE
    â”‚   â”‚                                  + temporal_config + compute metrics)
    â”‚   â”œâ”€â”€ <video_stem>_vis_data.npz     (tensors: pred_xyz (S,64,3),
    â”‚   â”‚                                  gt_future_xyz (64,3),
    â”‚   â”‚                                  ego_history_xyz (16,3),
    â”‚   â”‚                                  pred_rot, gt_future_rot,
    â”‚   â”‚                                  ego_history_rot, image_frames,
    â”‚   â”‚                                  camera_indices)
    â”‚   â””â”€â”€ <video_stem>_overlay.mp4      (H.264/CRF=18 trajectory overlay
    â”‚                                      with BEV inset + progressive reveal)
    â”œâ”€â”€ inference_summary.json            (all results array, CLI mode)
    â””â”€â”€ <run_id>_report.txt               (aggregate metrics report, workflow)

Aggregate Report Metrics:
  - Average / Total inference time
  - Average / Maximum GPU memory peak (GB)
  - Average / Maximum RAM peak (MB)
  - Average / Best / Worst minADE (metres)
  - Per-video breakdown: status, times, GPU/RAM, minADE, clip_id, camera

Performance:
  - Timing is recorded per-video in the inference JSON output
    (inference_time_seconds, total_time_seconds) and in the
    aggregate report (<run_id>_report.txt)
  - Model is loaded once and reused across all videos in a batch


COMPONENT G: UTILITIES & HELPERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

build_fluxfill_inpaint_csv.py:
  Purpose: Manual dataset creation from existing images
  Features:
    - Auto-captioning with Qwen VLM
    - Synthetic mask generation (rect/brush)
    - CSV assembly
    - Support for custom image collections

unzip_folder.py:
  Purpose: Extract video archives from GCS
  Use: Preprocessing Physical AI dataset downloads

VPData_download.py:
  Purpose: Download VideoPainter's VPData dataset
  Features:
    - Hugging Face integration
    - Parallel downloads
    - Checksum validation

================================================================================
5. IMPLEMENTATION DETAILS
================================================================================

PROGRAMMING LANGUAGES & FRAMEWORKS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Python 3.10
- PyTorch 2.1+
- Diffusers 0.31.0.dev0
- Transformers (latest from source)
- Accelerate
- PEFT (Parameter-Efficient Fine-Tuning)

DEEP LEARNING MODELS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. SAM2.1 (Segment Anything Model 2)
   - Architecture: Hiera-Large transformer
   - Parameters: 305M
   - Checkpoint: sam2.1_hiera_large.pt
   - Task: Video object segmentation
   - Input: RGB frames (1024x1024 typical)
   - Output: Binary segmentation masks

2. FluxFill (FLUX.1 Fill)
   - Architecture: Diffusion transformer
   - Provider: Black Forest Labs
   - Task: Image inpainting
   - Input: RGB image + binary mask + text prompt
   - Output: Inpainted RGB image
   - Resolution: 1024x1024 (native)

3. CogVideoX-5B-I2V
   - Architecture: 3D transformer (video DiT)
   - Parameters: 5B
   - Task: Image-to-video generation
   - Input: First frame + mask
   - Output: 49-frame video (6s at 8fps)
   - Resolution: 720x480 (typical)

4. VideoPainter Branch Model
   - Architecture: Context encoder
   - Parameters: ~300M (6% of CogVideoX)
   - Task: Background context injection
   - Integration: Plug-and-play with CogVideoX

5. Qwen2.5-VL (Vision Language Model)
   - Variants: 7B-Instruct, 72B-Instruct
   - Architecture: Multimodal transformer
   - Task: Visual understanding + caption generation
   - Input: RGB image + text prompt
   - Output: Structured text responses

6. Alpamayo-R1-10B (Vision-Language-Action Model) [NEW]
   - Architecture: Multimodal transformer with trajectory diffusion head
   - Parameters: 10B
   - Provider: NVIDIA
   - Task: Autonomous driving trajectory prediction with reasoning
   - Input: Multi-camera images (7 views Ã— 4 frames) + ego-motion history
   - Output: Future trajectory predictions (S Ã— 64 Ã— 3) + chain-of-thought
   - Inference: bfloat16 with flash-attention
   - Dataset: NVIDIA PhysicalAI-Autonomous-Vehicles (HuggingFace streaming)

HARDWARE REQUIREMENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Development:
  - GPU: NVIDIA A100 80GB (primary)
  - RAM: 64GB minimum
  - Storage: 500GB+ for checkpoints and datasets

Cloud Deployment (HLX):
  - Node: DedicatedNode(A100_80GB_1GPU or A100_80GB_2GPU)
  - Ephemeral storage: Maximum
  - Duration: Up to 3 days per task

GPU Memory Allocation:
  Single GPU Setup:
    - Models are loaded sequentially to manage VRAM
    - Explicit model unloading between stages (_unload_qwen_model)
    - VP_UNLOAD_QWEN_AFTER_USE=1 for single-GPU memory management
  
  Dual GPU Setup:
    - GPU 0: FluxFill + CogVideoX (sequential)
    - GPU 1: Qwen VLM
    - Parallel execution enabled
  
  Note: Actual VRAM usage depends on model configuration and batch size.
        The code tracks GPU memory via torch.cuda.memory_allocated() in
        Alpamayo and VPVideoMetrics.

CLOUD INFRASTRUCTURE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Platform: Google Cloud Platform (GCP)
Region: [REDACTED]
Container Registry: Artifact Registry ([REDACTED])

GCS Bucket Structure:
  [GCS_BUCKET]/
    â””â”€â”€ workspace/user/[USERNAME]/
        â”œâ”€â”€ Input/
        â”‚   â””â”€â”€ data_physical_ai/          (source driving videos)
        â”‚       â””â”€â”€ camera_front_tele_30fov/
        â”‚           â”œâ”€â”€ chunk_0000/
        â”‚           â”œâ”€â”€ chunk_0001/
        â”‚           â””â”€â”€ ... chunk_0199/
        â”œâ”€â”€ outputs/
        â”‚   â”œâ”€â”€ sam2/<run_id>/             (SAM2 raw outputs)
        â”‚   â”œâ”€â”€ preprocessed_data_vp/<run_id>/ (VideoPainter input)
        â”‚   â”œâ”€â”€ vp/<run_id>/              (VideoPainter edited videos)
        â”‚   â”‚   â””â”€â”€ <instruction_folder>/
        â”‚   â”‚       â””â”€â”€ <video_id>/
        â”‚   â”‚           â”œâ”€â”€ <video_id>_vp_edit_sample0.mp4
        â”‚   â”‚           â”œâ”€â”€ <video_id>_vp_edit_sample0.json
        â”‚   â”‚           â”œâ”€â”€ eval_<video_id>.txt
        â”‚   â”‚           â””â”€â”€ <video_id>_caption_refine/
        â”‚   â””â”€â”€ alpamayo/<run_id>/         (Alpamayo predictions)
        â”‚       â”œâ”€â”€ <video_stem>/
        â”‚       â”‚   â”œâ”€â”€ <video_stem>_inference.json
        â”‚       â”‚   â”œâ”€â”€ <video_stem>_vis_data.npz
        â”‚       â”‚   â””â”€â”€ <video_stem>_overlay.mp4
        â”‚       â””â”€â”€ <run_id>_report.txt
        â””â”€â”€ Video_inpainting/
            â”œâ”€â”€ sam2_checkpoint/
            â”‚   â””â”€â”€ checkpoints/
            â”‚       â””â”€â”€ sam2.1_hiera_large.pt
            â”œâ”€â”€ videopainter/
            â”‚   â”œâ”€â”€ ckpt/                  (Model checkpoints)
            â”‚   â”‚   â”œâ”€â”€ CogVideoX-5b-I2V/
            â”‚   â”‚   â”œâ”€â”€ flux_inp/
            â”‚   â”‚   â”œâ”€â”€ VideoPainter/
            â”‚   â”‚   â””â”€â”€ vlm/
            â”‚   â”‚       â”œâ”€â”€ Qwen2.5-VL-7B-Instruct/
            â”‚   â”‚       â””â”€â”€ Qwen2.5-VL-72B-Instruct/
            â”‚   â””â”€â”€ training/
            â”‚       â”œâ”€â”€ data/              (FluxFill datasets)
            â”‚       â””â”€â”€ trained_checkpoint/ (LoRA outputs)
            â””â”€â”€ vla/
                â””â”€â”€ alpamayo/
                    â””â”€â”€ checkpoints/
                        â””â”€â”€ alpamayo-r1-10b/  (Alpamayo model)

DOCKER CONTAINERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Master Orchestrator Container (NEW)
   Base: python:3.10-slim
   Purpose: Lightweight workflow graph serialisation (no GPU)
   Additional:
     - google-cloud-storage
     - gcsfs
     - hlx-wf
   
2. SAM2 Container
   Base: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
   Additional:
     - SAM2 installation (pip install -e .)
     - ffmpeg
     - gcsfs
     - transformers (for Qwen)
     - qwen-vl-utils
   
3. VideoPainter Container
   Base: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
   Additional:
     - Custom diffusers fork (VideoPainter branch)
     - CogVideoX dependencies
     - FluxFill dependencies
     - transformers (for Qwen)
     - accelerate
     - peft

4. Alpamayo VLA Container (NEW)
   Base: nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04
   Python: 3.12 (via deadsnakes PPA)
   Additional:
     - torch 2.8.0
     - transformers 4.57.1
     - flash-attn >= 2.8.3
     - physical_ai_av >= 0.1.0
     - accelerate, einops, av, hydra-core
     - alpamayo_r1 (editable install from src/)
     - hlx-wf
   Note: Patches physical_ai_av egomotion.py for writable numpy arrays

IMAGE TAGGING STRATEGY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Convention: {base_name}{run_suffix}:{MASTER_RUN_ID}
  where MASTER_RUN_ID = {RUN_ID}_{TIMESTAMP}

Examples:
  - harimt_sam2:002_20260217_095447
  - harimt_vpwithoutlora_5prompt_20260212_151908:002_20260217_095447
  - alpamayo_vla_002_20260217_095447:002_20260217_095447
  - master_pipeline:002_20260217_095447

Registry: europe-west4-docker.pkg.dev/mb-adas-2015-p-a4db/research/

Docker Image Names:
  Master:   master_pipeline
  SAM2:     harimt_sam2
  VP:       harimt_vp{VP_RUN_SUFFIX}
  Alpamayo: alpamayo_vla_{MASTER_RUN_ID}

Purpose: 
  - Shared MASTER_RUN_ID tag across all images for a given pipeline run
  - VP_RUN_SUFFIX encodes trained checkpoint timestamp for traceability
  - Alpamayo image name includes MASTER_RUN_ID for unique identification
  - SKIP_BUILD=1 mode allows reusing previously pushed images
  - Each image also tagged with :latest for convenience

================================================================================
6. WORKFLOW ORCHESTRATION (HLX)
================================================================================

HLX WORKFLOW SYSTEM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HLX (Helix) is a proprietary workflow orchestration platform for ML workloads.

Key Features:
- Declarative task definitions with @task decorator
- FuseBucket for efficient GCS mounting (FUSE-based)
- Dedicated compute node allocation
- Automatic retry and error handling
- Parameter passing and environment configuration

WORKFLOW DEFINITIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

0. Master Pipeline Workflow (NEW)
   File: workflow_master.py
   Entry: master_pipeline_wf
   
   Chains three stages sequentially with shared run_id:
   
   @task sam2_stage:
     compute: DedicatedNode(A100_80GB_1GPU)
     container_image: SAM2_CONTAINER_IMAGE
     mounts: FuseBucket (SAM2 checkpoints)
     Calls: workflow_sam2.run_sam2_segmentation()
   
   @task vp_stage:
     compute: DedicatedNode(A100_80GB_1GPU)
     container_image: VP_CONTAINER_IMAGE
     mounts: FuseBucket (VP ckpt, data, trained FluxFill, VLM 7B)
     environment: VP_COG_DEVICE, VP_FLUX_DEVICE, VP_QWEN_DEVICE,
                  VP_UNLOAD_QWEN_AFTER_USE, TRAINED_FLUXFILL_GCS_PATH
     Calls: workflow_vp.run_videopainter_edit_many()
   
   @task alpamayo_stage:
     compute: DedicatedNode(A100_80GB_1GPU)
     container_image: ALPAMAYO_CONTAINER_IMAGE
     mounts: FuseBucket (Alpamayo ckpt, VP output data)
     environment: HF_TOKEN, PYTORCH_CUDA_ALLOC_CONF
     Calls: workflow_alpamayo.run_alpamayo_inference_task()

1. SAM2 Segmentation Workflow
   File: segmentation/sam2/workflow_sam2.py (RENAMED from workflow.py)
   Entry: sam2_segmentation_wf
   
   @task Configuration:
     compute: DedicatedNode(A100_80GB_1GPU)
     container_image: [SAM2_CONTAINER]
     mounts:
       - FuseBucket (SAM2 checkpoints)
     environment:
       PYTHONUNBUFFERED: "1"
   
   Parameters:
     - run_id: str
     - sam2_video_uris: str (default/chunks:///gs:///comma-separated)
     - checkpoint_path: str (default: mounted checkpoint)
     - model_config: str
     - upload_to_gcp: bool = True
     - upload_to_local: bool = False
     - max_frames: int = 150
   
   New Features:
     - chunks:// URI resolution: _resolve_chunk_uri()
       Format: chunks://<bucket>/<prefix>?start=N&end=M&per_chunk=K
       Resolves chunk_NNNN/ subfolders via gcsfs.ls()
     - GCS video download with local caching (/tmp/sam2_video_cache)
     - Dual output: raw outputs + VP-preprocessed format
     - Output paths: gs://â€¦/outputs/sam2/<run_id>/ and
                     gs://â€¦/outputs/preprocessed_data_vp/<run_id>/

2. FluxFill Dataset Generation Workflow
   File: segmentation/sam2/data_generation.py
   Entry: fluxfill_data_generation_wf
   (unchanged from previous version)

3. Dataset Filtering Workflow
   File: segmentation/sam2/filter_fluxfill_dataset.py
   Entry: filter_fluxfill_dataset_wf
   (unchanged from previous version)

4. FluxFill LoRA Training Workflow
   File: generation/VideoPainter/training_workflow.py
   Entry: fluxfill_lora_training_wf
   (unchanged from previous version)

5. VideoPainter Editing Workflow
   File: generation/VideoPainter/workflow_vp.py (RENAMED from workflow.py)
   Entry: videopainter_many_wf
   
   @task Configuration:
     compute: DedicatedNode(A100_80GB_1GPU)
     container_image: [VP_CONTAINER]
     mounts:
       - FuseBucket (VideoPainter ckpt)
       - FuseBucket (preprocessed data)
       - FuseBucket (trained FluxFill LoRA)
       - FuseBucket (Qwen VLM 7B)
     environment:
       VP_COG_DEVICE: "cuda:0"
       VP_FLUX_DEVICE: "cuda:0"
       VP_QWEN_DEVICE: "auto"
       VP_UNLOAD_QWEN_AFTER_USE: "1"
   
   New Features:
     - Multi-instruction support:
       * Multiple instructions via || delimiter or newlines
       * Each instruction creates separate output subfolder
       * lane_specs shorthand: "lane {count} {color} {pattern}"
     - Per-video numerical evaluation:
       * PSNR, SSIM, LPIPS, MSE, MAE, Temporal Consistency, CLIP Score
       * Full-frame and masked-region metrics
       * Automatic eval_{video_id}.txt upload
     - VPVideoMetrics dataclass:
       * Tracks generation time, upload time, GPU memory, RSS
       * Written to per-instruction report file
     - Sidecar artifact upload:
       * FluxFill debug images (*_flux_i_img.png, *_flux_o_img.png)
       * Caption refinement logs (*_caption_refine/)
       * Generated-only video (*_generated.mp4)
     - Source file archival for reproducibility

6. Alpamayo VLA Inference Workflow (NEW)
   File: vla/alpamayo/workflow_alpamayo.py
   Entry: alpamayo_vla_inference_wf
   
   @task Configuration:
     compute: DedicatedNode(A100_80GB_1GPU)
     container_image: [ALPAMAYO_CONTAINER]
     mounts:
       - FuseBucket (Alpamayo checkpoints)
       - FuseBucket (VP output video data)
     environment:
       PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
       HF_HOME: "/root/.cache/huggingface"
       HF_TOKEN: (passed from environment)
   
   Parameters:
     - video_data_gcs_path: str (GCS path to VP edited videos)
     - output_run_id: str
     - model_id: str (default: mounted checkpoint or HF repo)
     - num_traj_samples: int = 1
     - video_name: str = "auto" (filter by stem, or all)
   
   Process:
     1. Symlink FuseBucket checkpoint â†’ /workspace/alpamayo/checkpoints
     2. Discover video files via FuseBucket mount (exclude _generated.mp4)
     3. Load Alpamayo model ONCE (bfloat16, flash-attn)
     4. For each video:
        a. Parse clip_id + camera_name from filename
        b. Decode VP frames (last 4 at native fps)
        c. Load PhysicalAI-AV dataset with matched time_step
        d. Replace camera frames with VP output
        e. Run VLA inference â†’ trajectories + reasoning
        f. Compute minADE vs ground truth
        g. Save JSON + NPZ + overlay video
     5. Write aggregate report
     6. Upload all outputs to GCS via gcsfs

FUSEBUCKET MOUNTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Advantage: FUSE-based lazy loading, no full dataset download
Mount Point: /mnt/{mount_name}

Example:
  FuseBucket(
    name="sam2-checkpoints",
    bucket="[GCS_BUCKET]",
    prefix="workspace/user/[USERNAME]/Video_inpainting/sam2_checkpoint",
  )
  
  Mounted at: /mnt/sam2-checkpoints/
  Access: /mnt/sam2-checkpoints/checkpoints/sam2.1_hiera_large.pt

WORKFLOW EXECUTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Command: hlx wf run {workflow_module}.{workflow_function}

Master Pipeline (recommended):
  bash scripts/build_and_run.sh
  # or with overrides:
  RUN_ID=002 SAM2_CHUNK_END=9 bash scripts/build_and_run.sh

Individual Workflows:
  # SAM2 only:
  cd segmentation/sam2 && bash scripts/build_and_run.sh
  
  # VideoPainter only:
  cd generation/VideoPainter && bash scripts/build_and_run.sh
  
  # Alpamayo only:
  cd vla/alpamayo && HF_TOKEN="hf_xxx" bash scripts/build_and_run.sh

  # Direct hlx invocation:
  hlx wf run workflow_master.master_pipeline_wf \
    --team-space research --domain prod \
    --run_id "002_20260217_095447" \
    --sam2_video_uris "chunks://bucket/prefix?start=0&end=19&per_chunk=1" \
    --vp_video_editing_instructions "Single solid white continuous line..."

Environment Variables (master build_and_run.sh):
  - RUN_ID: Base run identifier (default: 002)
  - SKIP_BUILD: Set to 1 to reuse pushed images
  - SAM2_CHUNK_START/END: Chunk range (default: 0-19)
  - SAM2_FILES_PER_CHUNK: Files per chunk (default: 1)
  - SAM2_MAX_FRAMES: Max frames per video (default: 150)
  - VIDEO_EDITING_INSTRUCTIONS: Multi-line or || delimited
  - VP_NUM_INFERENCE_STEPS: CogVideoX steps (default: 70)
  - VP_GUIDANCE_SCALE: Guidance scale (default: 6.0)
  - VP_STRENGTH: Inpainting strength (default: 1.0)
  - VP_CAPTION_REFINE_ITERS: VLM refinement (default: 10)
  - VP_CAPTION_REFINE_TEMPERATURE: VLM temperature (default: 0.1)
  - VP_DILATE_SIZE: Mask dilation (default: 24)
  - VP_MASK_FEATHER: Mask feathering (default: 8)
  - VP_IMG_INPAINTING_LORA_SCALE: LoRA scale (default: 0.0)
  - VP_SEED: Random seed (default: 42)
  - TRAINED_FLUXFILL_GCS_PATH: Trained LoRA checkpoint
  - HF_TOKEN: HuggingFace token (needed by Alpamayo)
  - ALPAMAYO_MODEL_ID: Alpamayo model path
  - ALPAMAYO_NUM_TRAJ_SAMPLES: Trajectory samples (default: 1)
  - ALPAMAYO_VIDEO_NAME: Video filter (default: auto)

BUILD SCRIPTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Master Pipeline (scripts/build_and_run.sh):
  1. Builds 4 Docker images: SAM2, VP, Alpamayo, Master
  2. Tags all with shared MASTER_RUN_ID
  3. Pushes to Google Artifact Registry
  4. Exports container image env vars
  5. Submits master_pipeline_wf via hlx wf run
  
  Features:
    - SKIP_BUILD=1 to reuse images
    - Auto-generates MASTER_RUN_ID = RUN_ID + timestamp
    - Prints comprehensive configuration summary
    - SAM2_EXPECTED_VIDEOS calculation for display

Alpamayo Standalone (vla/alpamayo/scripts/build_and_run.sh):
  1. Auto-discovers VP output folder matching RUN_ID
     (gcloud storage ls + grep for matching prefix)
  2. Builds and pushes Alpamayo image with RUN_TAG in name
  3. Requires HF_TOKEN for PhysicalAI-AV streaming
  4. Submits alpamayo_vla_inference_wf

Example (Master):
  #!/bin/bash
  set -euo pipefail
  
  RUN_ID="${RUN_ID:-002}"
  MASTER_RUN_ID="${RUN_ID}_$(date +%Y%m%d_%H%M%S)"
  
  # Build all images
  pushd segmentation/sam2 && docker compose build && popd
  pushd generation/VideoPainter && docker compose build && popd
  pushd vla/alpamayo && docker compose build && popd
  docker compose build  # master
  
  # Tag and push all
  docker tag sam2/frontend "${SAM2_TAGGED}"
  docker push "${SAM2_TAGGED}" ...
  
  # Submit master workflow
  hlx wf run workflow_master.master_pipeline_wf \
    --run_id "${MASTER_RUN_ID}" \
    --sam2_video_uris "${SAM2_VIDEO_URIS}" \
    ...

================================================================================
7. DATA GENERATION & PROCESSING
================================================================================

PHYSICAL AI DATASET
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Source: Google Cloud Storage / NVIDIA PhysicalAI-Autonomous-Vehicles
Path: gs://[GCS_BUCKET]/datasets/public/physical_ai_av/camera/camera_front_tele_30fov/
Format: MP4 videos from autonomous vehicle cameras
Organization: 200 chunks (chunk_0000 to chunk_0199)

Video Naming: {uuid}.camera_front_tele_30fov.mp4
Example: 25534c8d-4d02-463a-84c9-dad015f320ac.camera_front_tele_30fov.mp4

Note: Video resolution, FPS, duration, and total count are properties of
the NVIDIA PhysicalAI-AV dataset. The pipeline auto-detects FPS per video.

DATASET GENERATION STATISTICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Source: chunk_0000 to chunk_0199 (200 chunks available)
Frames per Video: 6 (frame numbers: 1, 100, 200, 300, 400, 500)
Filtering: Applied REQUIRE_CLEAR_ROAD=1

Note: Actual video counts, total samples, and final filtered dataset size
depend on the number of chunks processed in each run. The generation script
logs per-chunk statistics during execution.

Dataset Size:
  Depends on number of chunks processed and frames per video.
  Generated data is stored on GCS; actual size varies by run.

LANE ATTRIBUTE DISTRIBUTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Note: Distribution statistics can be computed by parsing the train.csv
prompts using the regex in filter_fluxfill_dataset.py. No pre-computed
distribution analysis exists in the codebase. The filter script can be
used to count samples matching each attribute combination.

FILTERED DATASETS CREATED
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
The filter_fluxfill_dataset.py script supports creating datasets filtered
by any combination of count/color/pattern attributes. Example configurations:

1. single_white_solid_clearroad
   - Count: single
   - Color: white
   - Pattern: solid
   - Clear road: Yes
   - Use: Primary training dataset

2. single_white_dashed_clearroad
   - Count: single
   - Color: white
   - Pattern: dashed

3. double_yellow_solid_clearroad
   - Count: double
   - Color: yellow
   - Pattern: solid

Note: Actual sample counts depend on the source dataset and attribute
      distribution. Use the filter script to determine counts.

DATA QUALITY CHECKS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Automated Validation:
  âœ“ Image file exists and readable
  âœ“ Mask file exists and readable
  âœ“ Image and mask dimensions match
  âœ“ Mask contains non-zero pixels
  âœ“ Caption matches structured format
  âœ“ No "unknown" attributes (if REQUIRE_CLEAR_ROAD=1)

Manual Validation (Sample):
  âœ“ Mask correctly covers road region
  âœ“ Caption semantically accurate
  âœ“ Image quality sufficient for training
  âœ“ No corrupt or truncated files

================================================================================
8. TRAINING PIPELINE
================================================================================

FLUXFILL LORA TRAINING CONFIGURATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dataset: single_white_solid_clearroad_10000
Base Model: FLUX.1 Fill (black-forest-labs/FLUX.1-Fill-dev)
Training Method: LoRA (Low-Rank Adaptation)

LoRA Hyperparameters:
  rank: 128
  alpha: 128
  target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
  lora_dropout: 0.0

Training Hyperparameters:
  max_train_steps: Empty (trains on full dataset)
  num_train_epochs: 5
  learning_rate: 1e-5
  lr_scheduler: "cosine"
  lr_warmup_steps: 50
  train_batch_size: 1
  gradient_accumulation_steps: 4
  mixed_precision: "bf16"
  optimizer: AdamW
  weight_decay: 1e-2
  max_grad_norm: 1.0

Checkpointing:
  checkpointing_steps: 100
  checkpoint_total_limit: 4
  resume_from_checkpoint: latest (if exists)

Hardware:
  GPU: A100 80GB
  VRAM Usage: depends on model configuration
  Training Time: recorded in training_summary.txt output

Output:
  GCS Path: gs://[GCS_BUCKET]/training/trained_checkpoint/fluxfill_single_white_solid_clearroad_YYYYMMDD_HHMMSS/
  Checkpoint Structure:
    checkpoint-100/
      adapter_config.json
      adapter_model.safetensors
      optimizer.bin
      scheduler.bin
    checkpoint-200/
    checkpoint-300/
    ...
  
  Note: Checkpoint naming automatically includes timestamp for traceability

TRAINING LOGS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Training logs are saved by Accelerate to the output directory.
The training script records:
  - Per-step loss to loss_history.json
  - Training summary (duration, final loss, dataset size) to training_summary.txt
  - Checkpoints saved every checkpointing_steps intervals

Use tensorboard or wandb for detailed monitoring.

LORA WEIGHTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Base Model: FLUX.1 Fill (black-forest-labs/FLUX.1-Fill-dev)
LoRA targets: ["to_k", "to_q", "to_v", "to_out.0"] transformer blocks
Rank: 128, Alpha: 128

The LoRA adapter weights are significantly smaller than the base model,
enabling efficient storage and deployment. Exact sizes depend on the
final checkpoint.

INTEGRATION WITH VIDEOPAINTER
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Loading LoRA-trained FluxFill:
  
  from diffusers import FluxFillPipeline
  
  # Load base FluxFill model
  pipe = FluxFillPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-Fill-dev",
    torch_dtype=torch.bfloat16
  )
  
  # Load LoRA weights (automatically extracted from checkpoint timestamp)
  pipe.load_lora_weights(
    "gs://[GCS_BUCKET]/trained_checkpoint/fluxfill_single_white_solid_clearroad_YYYYMMDD_HHMMSS/checkpoint-900"
  )
  
  # Inference
  result = pipe(
    image=input_image,
    mask_image=mask,
    prompt="road with single white solid lane markings, crisp paint, centered alignment"
  )

Performance Impact:
  - Quality: Improved lane marking realism with trained LoRA vs. base model
  - Exact inference time and VRAM usage depend on resolution and model config
  
Note: The build_and_run.sh script automatically extracts the checkpoint 
      timestamp from the TRAINED_FLUXFILL_GCS_PATH for proper tracking

================================================================================
9. EVALUATION & RESULTS
================================================================================

EVALUATION METHODOLOGY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Automated VLM Evaluation (Qwen2.5-VL)
   - Semantic correctness (lane attributes match instruction)
   - Visual quality (texture, alignment, lighting)
   - Context preservation (background unchanged)
   - Structured JSON output (verdict + notes)

2. Manual Visual Inspection
   - Sample 100 generated videos
   - Rate on 1-5 scale for:
     * Semantic accuracy
     * Visual realism
     * Temporal consistency
     * Artifact presence

3. Quantitative Metrics (when applicable)
   - First-frame pass rate (VLM evaluation)
   - Average refinement iterations
   - Processing time per video

4. Automated Numerical Video Quality Metrics (NEW)
   Computed per-video via _evaluate_video() in workflow_vp.py using
   MetricsCalculator from evaluate/metrics.py. Two passes per video:
   a) Full-frame metrics â€” compare entire original vs. generated frame
   b) Masked-region metrics â€” compare only within inpainted mask area

   Metrics (both full-frame and masked-region):
   - PSNR (Peak Signal-to-Noise Ratio): 10Â·log10(1/MSE) dB, higher=better
   - SSIM (Structural Similarity Index): luminance+contrast+structure [0,1]
   - LPIPS (Learned Perceptual Image Patch Similarity): deep perceptual
     distance, lower=better
   - MSE (Mean Squared Error): (1/N)Î£(x_i âˆ’ y_i)Â², lower=better
   - MAE (Mean Absolute Error): (1/N)Î£|x_i âˆ’ y_i|, lower=better
   - Temporal Consistency: frame-to-frame coherence, higher=better
   - CLIP Score: text-video alignment (only when caption provided)

   Quality Assessment (from full-frame metrics):
   - PSNR > 40 dB â†’ +1 point
   - SSIM > 0.95  â†’ +1 point
   - LPIPS < 0.1  â†’ +1 point
   - Score â‰¥ 2: EXCELLENT | Score = 1: GOOD | Score = 0: MODERATE

   Output: eval_{video_id}.txt saved alongside generated video in GCS
   Contains: video metadata, resolution, FPS, mask coverage %,
   full-frame metrics block, masked-region metrics block, quality label.

5. VLA Trajectory Impact Assessment (NEW â€” Alpamayo)
   - minADE (minimum Average Displacement Error): distance between
     predicted and ground-truth ego-vehicle trajectories (metres)
     Formula: min_s (1/T) Î£_t ||pred_xy^(s)_t âˆ’ gt_xy_t||_2
   - Computed per-video and aggregated across the run
     (average, best, worst minADE in report)
   - Ground truth from PhysicalAI-AV HuggingFace dataset
   - Per-video compute metrics: inference time, GPU memory, RAM usage
   - Measures whether edited lane markings shift VLA driving behaviour
   - Trajectory overlay visualization with BEV inset for qualitative eval

RESULTS: FLUXFILL FIRST-FRAME GENERATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Note: Systematic quantitative evaluation is pending. The VLM refinement
loop records per-video results (qwen_eval.json, caption_refine/ artifacts)
that can be post-processed to compute pass rates and iteration statistics.

The per-video sidecar JSON files contain:
  - Caption_refine_iterations_used
  - Caption_refine_iters_requested
  - VLM verdict (PASS/FAIL) per iteration
  - Revised captions and reasoning

Qualitative observations:
  - LoRA-trained FluxFill produces lane markings that more closely match
    the requested attributes (count, color, pattern) vs. base FluxFill
  - Fewer VLM refinement iterations needed with LoRA
  - Improved paint texture realism and perspective alignment

PROCESSING PERFORMANCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Hardware: A100 80GB, single GPU

Note: Per-stage timing instrumentation exists in SAM2 (VideoTiming
dataclass in process_videos_sam2.py) and Alpamayo (inference_time_seconds
in run_inference.py). VideoPainter (edit_bench.py) tracks generation
time per video in VPVideoMetrics. Exact throughput numbers depend on
video length, number of refinement iterations, and model configuration.

Timing data is recorded in:
  - SAM2: per-video timing breakdown (segment, postprocess, upload)
  - VideoPainter: VPVideoMetrics.generation_s per video
  - Alpamayo: inference_time_seconds per video in JSON output
  - Alpamayo: aggregate avg/total time in <run_id>_report.txt

LIMITATIONS & KNOWN CHALLENGES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Complex Intersections:
   - Challenge: Multiple converging lanes
   - Observed: Reduced quality in multi-lane scenarios
   - Future work: Multi-line handling

2. Severe Occlusions:
   - Challenge: Vehicles covering road region
   - Observed: Mask generation may include non-road areas
   - Future work: Better mask generation or frame selection

3. Extreme Lighting:
   - Challenge: Strong shadows, nighttime scenes
   - Observed: Reduced inpainting quality under extreme lighting
   - Future work: Lighting-conditional training data

4. Curved Roads:
   - Challenge: Maintaining perspective on curves
   - Observed: Slight quality degradation on curved segments
   - Future work: Curve-specific data augmentation

Note: Systematic failure-rate analysis by scene category is pending.
      Per-video VLM verdicts and numerical metrics can be aggregated
      from the eval_{video_id}.txt and sidecar JSON files.

AUTOMATED NUMERICAL QUALITY METRICS (NEW)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Per-video metrics computed automatically by _evaluate_video() in workflow_vp.py
after each video generation. Uses MetricsCalculator on GPU tensors [T,C,H,W].

Metric                  | Full-Frame | Masked Region | Notes
------------------------|------------|---------------|------
PSNR (dB)               | âœ“          | âœ“             | 10Â·log10(1/MSE), higher=better
SSIM                    | âœ“          | âœ“             | luminance+contrast+structure [0,1]
LPIPS                   | âœ“          | âœ“             | deep perceptual dist, lower=better
MSE                     | âœ“          | âœ“             | (1/N)Î£(xâˆ’y)Â², lower=better
MAE                     | âœ“          | âœ“             | (1/N)Î£|xâˆ’y|, lower=better
Temporal Consistency    | âœ“          | âœ“             | frame-to-frame coherence
CLIP Score              | âœ“          | âœ“             | text-video alignment (if caption)

Quality Assessment (from full-frame metrics):
  Score = (PSNR>40) + (SSIM>0.95) + (LPIPS<0.1)
  â‰¥ 2 â†’ EXCELLENT | 1 â†’ GOOD | 0 â†’ MODERATE

Output format: eval_{video_id}.txt containing:
  - Video metadata (ID, paths, resolution, FPS, mask coverage %)
  - Full-frame metrics block (all 7 metrics)
  - Masked-region metrics block (all 7 metrics, mask > 0.5)
  - Quality assessment label
  Uploaded to GCS alongside the generated video.

VLA TRAJECTORY EVALUATION (NEW â€” ALPAMAYO)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Alpamayo-R1-10B trajectory prediction on edited vs. original videos:

  Metric: minADE (minimum Average Displacement Error, in metres)
  Formula: minADE = min_s (1/T) Î£_t ||pred_xy^(s)_t âˆ’ gt_xy_t||_2
  Computed over XY only (2D ground plane), T = 64 future waypoints
  
  Protocol:
  1. For each VP-edited video, parse clip_id and camera_name from filename
     Pattern: <clip_id>.<camera_name>_vp_edit_sample0.mp4
  2. Decode last 4 consecutive frames via PyAV; detect FPS â†’ time_step
  3. Stream matching clip from PhysicalAI-AV HuggingFace dataset
     using time_step = 1/fps for temporal alignment
  4. Replace original camera frames with VP-decoded frames
     (resize via bilinear interpolation if dimensions differ)
  5. Build multi-camera input: 7 cameras Ã— 4 frames = 28 images
  6. Run VLA inference (top_p=0.98, temperature=0.6, max_gen_len=256)
  7. Extract S trajectory samples (S Ã— 64 Ã— 3) + chain-of-thought traces
  8. Compute minADE vs ground-truth ego-motion (XY only)
  
  Output artifacts per video (<video_stem>/ subdirectory):
  - <video_stem>_inference.json:
      video_id, clip_id, camera_name, min_ade_meters,
      reasoning_traces (chain-of-thought), temporal_config
      (vp_video_fps, time_step_seconds, num_frames_per_camera),
      metrics (inference_time_seconds, gpu_memory_used/peak_gb,
      ram_used/peak_mb), success flag
  - <video_stem>_vis_data.npz:
      pred_xyz (S,64,3), pred_rot (S,64,3,3),
      gt_future_xyz (64,3), gt_future_rot (64,3,3),
      ego_history_xyz (16,3), ego_history_rot (16,3,3),
      image_frames (N_camÃ—T,H,W,3), camera_indices (N_cam,)
  - <video_stem>_overlay.mp4 (H.264, CRF=18, yuv420p):
      Trajectory overlay with pinhole projection on camera view
      + BEV inset (250Ã—250 px, bottom-left) + progressive reveal
      + HUD text (minADE, clip_id, camera) + legend box
  
  Aggregate report (<run_id>_report.txt, workflow mode):
  - Total/successful/failed video counts
  - Average inference time per video
  - Average/maximum GPU memory peak (GB)
  - Average/maximum RAM peak (MB)
  - Average/best/worst minADE (metres)
  - Per-video breakdown with status, timings, and clip metadata
  Written to GCS: outputs/alpamayo/<run_id>/<run_id>_report.txt

  CLI mode also produces inference_summary.json (array of all results).

  A higher minADE (compared to unedited baseline) indicates the edited lane
  markings caused the VLA to plan a different trajectory â€” quantifying the
  perceptual impact of the lane editing on autonomous driving behaviour.

  Visualization Technical Details:
  - Camera projection: ego frame (x=forward, y=left, z=up) â†’
    camera frame (z=forward, x=right, y=down)
  - Focal length: f_x = f_y = (W/2) / tan(FOV/2)
  - FOV auto-detected: 120Â° for *120fov*, 30Â° for *30fov*, 70Â° otherwise
  - GT drawn in red (thickness=3), predictions in 5 cycling colors
  - BEV display frame: dx=âˆ’y, dy=x (forward=up), auto-scaled
  - Ego history drawn in gray, white dot at ego origin
  - Batch mode: render_all_in_directory() scans *_vis_data.npz files

================================================================================
10. TECHNICAL SPECIFICATIONS
================================================================================

REPOSITORY STRUCTURE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
masterthesis_hk/
â”œâ”€â”€ workflow_master.py                      [NOVEL: Master pipeline orchestrator]
â”œâ”€â”€ Dockerfile                              [NOVEL: Lightweight orchestrator container]
â”œâ”€â”€ docker-compose.yaml                     [NOVEL: Master + optional local services]
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ build_and_run.sh                    [NOVEL: Master build/run script]
â”‚
â”œâ”€â”€ generation/
â”‚   â””â”€â”€ VideoPainter/
â”‚       â”œâ”€â”€ infer/
â”‚       â”‚   â”œâ”€â”€ edit_bench.py               [MODIFIED: FluxFill + VLM integration]
â”‚       â”‚   â”œâ”€â”€ edit_bench.sh
â”‚       â”‚   â”œâ”€â”€ edit.py
â”‚       â”‚   â”œâ”€â”€ inpaint.py
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ train/
â”‚       â”‚   â”œâ”€â”€ train_fluxfill_inpaint_lora.py  [NOVEL: LoRA training]
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ data_utils/
â”‚       â”‚   â”œâ”€â”€ build_fluxfill_inpaint_csv.py   [UTILITY: Dataset creation]
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ scripts/
â”‚       â”‚   â”œâ”€â”€ build_and_run.sh                [WORKFLOW: VP inference]
â”‚       â”‚   â”œâ”€â”€ build_and_run_training.sh       [WORKFLOW: LoRA training]
â”‚       â”‚   â””â”€â”€ download_physicalai_data.sh     [UTILITY: Data download]
â”‚       â”œâ”€â”€ workflow_vp.py                      [MODIFIED: HLX orchestration, renamed]
â”‚       â”œâ”€â”€ training_workflow.py                [NOVEL: Training workflow]
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ docker-compose.yaml
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ segmentation/
â”‚   â””â”€â”€ sam2/
â”‚       â”œâ”€â”€ process_videos_sam2.py              [NOVEL: SAM2 road segmentation]
â”‚       â”œâ”€â”€ process_vide_sam2_hlxwf.py          [NOVEL: Workflow wrapper]
â”‚       â”œâ”€â”€ workflow_sam2.py                    [NOVEL: SAM2 workflow, renamed]
â”‚       â”œâ”€â”€ data_generation.py                  [NOVEL: Dataset generation]
â”‚       â”œâ”€â”€ filter_fluxfill_dataset.py          [NOVEL: Dataset filtering]
â”‚       â”œâ”€â”€ scripts/
â”‚       â”‚   â”œâ”€â”€ build_and_run.sh                [WORKFLOW: SAM2 segmentation]
â”‚       â”‚   â”œâ”€â”€ build_and_run_training_data.sh  [WORKFLOW: Dataset generation]
â”‚       â”‚   â””â”€â”€ build_and_run_sorting_data.sh   [WORKFLOW: Dataset filtering]
â”‚       â”œâ”€â”€ sam2/                               [SAM2 library]
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ docker-compose.yaml
â”‚       â”œâ”€â”€ README_PIPELINE.md
â”‚       â””â”€â”€ README.md
â”‚
â””â”€â”€ vla/
    â””â”€â”€ alpamayo/                               [NOVEL: Entire component]
        â”œâ”€â”€ run_inference.py                    [NOVEL: VLA inference runner]
        â”œâ”€â”€ visualize_video.py                  [NOVEL: Trajectory overlay renderer]
        â”œâ”€â”€ workflow_alpamayo.py                [NOVEL: HLX workflow]
        â”œâ”€â”€ download_checkpoints.py             [UTILITY: Model download]
        â”œâ”€â”€ scripts/
        â”‚   â””â”€â”€ build_and_run.sh                [WORKFLOW: Alpamayo standalone]
        â”œâ”€â”€ src/
        â”‚   â””â”€â”€ alpamayo_r1/                    [Alpamayo model source]
        â”‚       â”œâ”€â”€ models/
        â”‚       â”‚   â””â”€â”€ alpamayo_r1.py
        â”‚       â”œâ”€â”€ load_physical_aiavdataset.py
        â”‚       â”œâ”€â”€ helper.py
        â”‚       â”œâ”€â”€ config.py
        â”‚       â”œâ”€â”€ geometry/
        â”‚       â”œâ”€â”€ diffusion/
        â”‚       â”œâ”€â”€ action_space/
        â”‚       â””â”€â”€ test_inference.py
        â”œâ”€â”€ notebooks/                          [Jupyter notebooks for analysis]
        â”œâ”€â”€ npz/                                [Visualization data cache]
        â”œâ”€â”€ checkpoints/                        [Model checkpoints (symlinked)]
        â”œâ”€â”€ Dockerfile                          [Python 3.12, CUDA 12.1]
        â”œâ”€â”€ docker-compose.yaml
        â”œâ”€â”€ pyproject.toml
        â””â”€â”€ README.md

KEY FILES SUMMARY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[NOVEL] = Created from scratch
[MODIFIED] = Significant modifications to existing code
[UTILITY] = Helper scripts

Novel Files (Created):
  1. workflow_master.py (427 lines)
     - Master pipeline orchestrator: SAM2 â†’ VP â†’ Alpamayo
     - Three @task functions + @workflow
     - Shared run_id propagation
  
  2. segmentation/sam2/process_videos_sam2.py (1016 lines)
     - Core SAM2 video processing logic
     - Road-specific point grid
     - VideoPainter output format
  
  3. segmentation/sam2/workflow_sam2.py (442 lines, renamed from workflow.py)
     - SAM2 HLX workflow with chunks:// URI resolution
     - GCS video caching and download
  
  4. segmentation/sam2/data_generation.py (964 lines)
     - End-to-end dataset generation pipeline
     - VLM integration for captions
     - GCS chunk-based processing
  
  5. segmentation/sam2/filter_fluxfill_dataset.py (725 lines)
     - Semantic attribute filtering
     - Multi-criteria dataset curation
     - Quality control logic
  
  6. generation/VideoPainter/train/train_fluxfill_inpaint_lora.py (440 lines)
     - LoRA training implementation
     - Custom CSV dataset loader
     - Accelerate integration
  
  7. generation/VideoPainter/training_workflow.py (371 lines)
     - HLX workflow for training
     - GCS checkpoint management
  
  8. vla/alpamayo/run_inference.py (469 lines)
     - Alpamayo VLA inference on VP-edited videos
     - clip_id/camera parsing from VP filenames
     - PhysicalAI-AV dataset loading with temporal alignment
     - minADE computation and visualization
  
  9. vla/alpamayo/workflow_alpamayo.py (550 lines)
     - HLX workflow for Alpamayo VLA inference
     - VLAVideoMetrics dataclass
     - Aggregate report generation
     - FuseBucket video data mounting
  
  10. vla/alpamayo/visualize_video.py (530 lines)
      - Trajectory overlay rendering on camera views
      - Pinhole camera projection (egoâ†’camera frame transform)
      - Bird's-eye-view (BEV) inset rendering (250Ã—250 px)
      - Progressive reveal mode (linear frameâ†’waypoint mapping)
      - H.264/MP4 output (CRF=18, yuv420p via PyAV)
      - Batch mode: render_all_in_directory() for directory scan
      - CLI: single and batch subcommands
  
  11. vla/alpamayo/download_checkpoints.py (checkpoint download utility)
  
  12. scripts/build_and_run.sh (300 lines)
      - Master build/run script: builds all 4 images + submits workflow
  
  13. vla/alpamayo/scripts/build_and_run.sh (standalone Alpamayo)
  
  14. Dockerfile (54 lines) â€” Master orchestrator container
  
  15. docker-compose.yaml (100 lines) â€” Master + optional local services
  
  16. All other workflow orchestration files (build_and_run_*.sh scripts)

Modified Files:
  1. generation/VideoPainter/infer/edit_bench.py (1639 lines)
     - Added FluxFillPipeline integration (~300 lines)
     - VLM refinement loop (~200 lines)
     - Multi-GPU orchestration (~100 lines)
     - Mask processing utilities (~100 lines)
  
  2. generation/VideoPainter/workflow_vp.py (1401 lines, renamed from workflow.py)
     - Multi-instruction support (|| delimited)
     - Per-video numerical evaluation (PSNR/SSIM/LPIPS/CLIP)
     - VPVideoMetrics and run reporting
     - Sidecar artifact upload
     - Caption refinement artifact archival
     - Source file upload for reproducibility
     - _evaluate_video() function for automated metrics

CONFIGURATION FILES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0. Dockerfile (root â€” Master Orchestrator)
   Base: python:3.10-slim
   Installs: gcsfs, google-cloud-storage, hlx-wf wheel
   Copies: workflow_master.py
   Entrypoint: sleep infinity (HLX runs workflow internally)

0b. docker-compose.yaml (root â€” Master Pipeline)
   Services: master-pipeline-service
   Volumes: /workspace mapped
   Environment: RUN_ID, GCS paths, container image refs

1. segmentation/sam2/Dockerfile
   Base: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
   Installs: SAM2 (editable), gcsfs, hlx-wf wheel, av, decord
   Copies: workflow_sam2.py, process_videos_sam2.py, etc.

1b. segmentation/sam2/docker-compose.yaml
   Services: frontend (SAM2 + transformers)
   Build: FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
   
2. generation/VideoPainter/Dockerfile
   Base: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
   Installs: custom diffusers, transformers, accelerate, peft, gcsfs
   Copies: workflow_vp.py, infer/, train/, scripts/

2b. generation/VideoPainter/docker-compose.yaml
   Services: frontend (VideoPainter + diffusers)
   Build: FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
   
3. generation/VideoPainter/requirements.txt
   Key dependencies:
     - diffusers @ git+https://github.com/huggingface/diffusers.git
     - transformers @ git+https://github.com/huggingface/transformers.git
     - accelerate
     - peft
     - torch>=2.1.0
     - torchvision
     - opencv-python
     - pillow
     - gcsfs

4. vla/alpamayo/Dockerfile (NEW)
   Base: nvidia/cuda:12.1.1-devel-ubuntu22.04
   Python: 3.12 (built from deadsnakes PPA)
   Installs: torch==2.8.0 (cu121), transformers==4.57.1, flash-attn>=2.8.3,
             physical_ai_av, einops, hydra-core, av, gcsfs, hlx-wf wheel
   Patches: egomotion.py for writable numpy arrays (np.asarray â†’ np.array)
   Copies: workflow_alpamayo.py, run_inference.py, visualize_video.py,
           download_checkpoints.py, src/

4b. vla/alpamayo/docker-compose.yaml (NEW)
   Services: alpamayo-inference
   Runtime: nvidia (GPU passthrough)
   Volumes: checkpoints, data mounts
   Environment: HF_TOKEN, MODEL_ID, OUTPUT_DIR

ENVIRONMENT VARIABLES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Master Pipeline:
  RUN_ID                          Base run identifier (default: 002)
  SKIP_BUILD                      Set to 1 to reuse pushed images
  SAM2_CHUNK_START/END            Chunk range for SAM2 input
  SAM2_FILES_PER_CHUNK            Files per chunk (default: 1)
  SAM2_MAX_FRAMES                 Max frames per video (default: 150)
  VIDEO_EDITING_INSTRUCTIONS      Multi-line or || delimited
  HF_TOKEN                        HuggingFace token for Alpamayo

SAM2 Workflows:
  SAM2_CONTAINER_IMAGE        Container image override
  SAM2_OUTPUT_BASE            GCS base for raw SAM2 outputs
  SAM2_PREPROCESSED_OUTPUT_BASE  GCS base for VP-preprocessed outputs

VideoPainter Workflows:
  VP_CONTAINER_IMAGE          Container image override (with timestamp tag)
  VP_RUN_SUFFIX               Image naming suffix (extracted from checkpoint timestamp)
  DATA_RUN_ID                 SAM2 output run identifier to use
  VP_OUTPUT_BASE              GCS base for VP outputs
  LLM_MODEL_SIZE              Qwen model size (7B/72B)
  TRAINED_FLUXFILL_GCS_PATH   Path to trained LoRA checkpoint
  VP_COG_DEVICE               CogVideoX device (default: cuda:0)
  VP_FLUX_DEVICE              FluxFill device (default: cuda:0)
  VP_QWEN_DEVICE              Qwen VLM device (default: auto)
  VP_UNLOAD_QWEN_AFTER_USE    Unload VLM after caption (default: 1)

Alpamayo Workflows:
  ALPAMAYO_CONTAINER_IMAGE    Container image override
  ALPAMAYO_OUTPUT_BASE        GCS prefix for outputs (no gs:// scheme)
  ALPAMAYO_VIDEO_DATA_PREFIX  GCS prefix where VP videos live
  HF_TOKEN                    HuggingFace token for PhysicalAI-AV streaming

Training:
  INPUT_DATA_DIR              GCS dataset path
  OUTPUT_CHECKPOINT_DIR       GCS output path
  OUTPUT_RUN_ID               Training run identifier (auto-generated with timestamp)
  MAX_TRAIN_STEPS             Max training steps (empty = train on full dataset)
  NUM_TRAIN_EPOCHS            Number of epochs (default: 5)
  CHECKPOINTING_STEPS         Checkpoint frequency (default: 100)
  LEARNING_RATE               Learning rate (default: 1e-5)
  GRAD_ACCUM                  Gradient accumulation steps (default: 4)

Inference:
  CAPTION_REFINE_ITERS        VLM refinement iterations (default: 10)
  CAPTION_REFINE_TEMPERATURE  VLM temperature (default: 0.1)
  VP_STRENGTH                 Inpainting strength (default: 1.0)
  VP_NUM_INFERENCE_STEPS      CogVideoX steps (default: 70)
  VP_GUIDANCE_SCALE           Classifier-free guidance (default: 6.0)
  VP_DILATE_SIZE              Mask dilation pixels (default: 24)
  VP_MASK_FEATHER             Mask feather pixels (default: 8)
  VP_KEEP_MASKED_PIXELS       Preserve original masked pixels (default: True)
  VP_IMG_INPAINTING_LORA_SCALE  LoRA scale (default: 0.0)

COMMAND-LINE INTERFACES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0. Master Pipeline (End-to-End):
   bash scripts/build_and_run.sh
   
   # With overrides:
   RUN_ID=003 \
   SAM2_CHUNK_START=0 SAM2_CHUNK_END=9 \
   VP_CAPTION_REFINE_ITERS=5 \
   HF_TOKEN="hf_xxx" \
   bash scripts/build_and_run.sh

1. SAM2 Video Processing:
   python process_videos_sam2.py \
     --input_uris video1.mp4 video2.mp4 \
     --output_dir /path/to/output \
     --max_frames 150

2. Dataset Generation:
   hlx wf run data_generation.fluxfill_data_generation_wf \
     -p source_gcs_prefix="..." \
     -p num_videos=10000 \
     -p chunk_start=0 \
     -p chunk_end=199

3. Dataset Filtering:
   python filter_fluxfill_dataset.py \
     --input_dir gs://[GCS_BUCKET]/dataset \
     --count single \
     --color white \
     --pattern solid

4. LoRA Training:
   python train_fluxfill_inpaint_lora.py \
     --train_data /path/to/train.csv \
     --pretrained_model_path /path/to/flux_inp \
     --output_dir /path/to/output \
     --num_train_epochs 5 \
     --learning_rate 1e-5 \
     --gradient_accumulation_steps 4 \
     --checkpointing_steps 100

   Note: MAX_TRAIN_STEPS can be left empty to train on full dataset for NUM_TRAIN_EPOCHS

5. VideoPainter Editing:
   python infer/edit_bench.py \
     --meta_csv /path/to/meta.csv \
     --video_root /path/to/videos \
     --mask_root /path/to/masks \
     --instruction "lane single white solid" \
     --output_dir /path/to/output \
     --llm_model /path/to/Qwen2.5-VL-7B-Instruct \
     --caption_refine_iters 10 \
     --caption_refine_temperature 0.1 \
     --strength 1.0

   Note: Environment variables can override defaults:
   CAPTION_REFINE_ITERS=5 VP_STRENGTH=0.85 bash scripts/build_and_run.sh

6. Alpamayo VLA Inference (NEW):
   # Standalone:
   python run_inference.py \
     --video_path /path/to/vp_edited_video.mp4 \
     --output_dir /path/to/output \
     --model_id nvidia/Alpamayo-R1-10B \
     --num_traj_samples 1
   
   # Via build script (auto-discovers VP output):
   HF_TOKEN="hf_xxx" bash scripts/build_and_run.sh
   
   # Via HLX workflow:
   hlx wf run workflow_alpamayo.alpamayo_vla_inference_wf \
     --video_data_gcs_path "gs://bucket/outputs/vp/run_001/" \
     --output_run_id "alp_run_001" \
     --model_id "/workspace/alpamayo/checkpoints/alpamayo-r1-10b" \
     --num_traj_samples 1 \
     --video_name "auto"

DEPENDENCIES & VERSIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Core (SAM2 + VideoPainter containers):
  Python: 3.10
  PyTorch: 2.1.0+cu118
  CUDA: 11.8
  cuDNN: 8

Core (Alpamayo container):
  Python: 3.12
  PyTorch: 2.8.0+cu121
  CUDA: 12.1
  flash-attn: >= 2.8.3

Core (Master Orchestrator container):
  Python: 3.10-slim (no CUDA)

Key Libraries (SAM2 + VideoPainter):
  diffusers: 0.31.0.dev0 (custom fork)
  transformers: latest (from source)
  accelerate: 0.25.0+
  peft: 0.7.0+
  opencv-python: 4.8.0+
  pillow: 10.0.0+
  gcsfs: 2023.9.0+
  numpy: 1.24.0+
  torch: 2.1.0+
  torchvision: 0.16.0+

Key Libraries (Alpamayo â€” NEW):
  transformers: 4.57.1
  physical_ai_av: >= 0.1.0 (NVIDIA PhysicalAI-AV dataset API)
  flash-attn: >= 2.8.3 (GPU-accelerated attention)
  einops: >= 0.8.0
  hydra-core: >= 1.3
  av: >= 16.0.1 (PyAV video decoder)
  psutil: >= 5.9
  gcsfs: 2023.9.0+
  scikit-image: >= 0.20.0 (SSIM computation)
  lpips: >= 0.1.4 (perceptual similarity)
  open_clip_torch: >= 2.0.0 (CLIP score)

Model Checkpoints:
  SAM2: sam2.1_hiera_large.pt (305M params)
  CogVideoX: CogVideoX-5b-I2V (5B params)
  VideoPainter: branch model (~300M params)
  FluxFill: FLUX.1-Fill-dev (2.8B params)
  Qwen: Qwen2.5-VL-7B-Instruct (7B params)
         Qwen2.5-VL-72B-Instruct (72B params)
  Alpamayo: Alpamayo-R1-10B (10B params, bfloat16) (NEW)

REPRODUCIBILITY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
To reproduce this work:

1. Clone repository
2. Set up GCS access (authenticate with gcloud)
3. Download checkpoints to GCS:
   - SAM2: sam2.1_hiera_large.pt
   - CogVideoX-5b-I2V
   - VideoPainter branch model
   - FluxFill (FLUX.1-Fill-dev)
   - Qwen2.5-VL-7B-Instruct
4. Run data generation pipeline:
   cd segmentation/sam2
   bash scripts/build_and_run_training_data.sh
5. Filter dataset:
   bash scripts/build_and_run_sorting_data.sh
6. Train LoRA (with improved hyperparameters):
   cd ../../generation/VideoPainter
   # Training runs for 5 epochs on full dataset by default
   # Output checkpoint: fluxfill_single_white_solid_clearroad_YYYYMMDD_HHMMSS
   bash scripts/build_and_run_training.sh
7. Run inference (automatically uses checkpoint timestamp):
   # Set TRAINED_FLUXFILL_GCS_PATH to your checkpoint location
   TRAINED_FLUXFILL_GCS_PATH="workspace/user/.../fluxfill_single_white_solid_clearroad_YYYYMMDD_HHMMSS" \
   bash scripts/build_and_run.sh
8. Download Alpamayo checkpoint (NEW):
   cd ../../vla/alpamayo
   python download_checkpoints.py   # downloads nvidia/Alpamayo-R1-10B
   # Or use HuggingFace CLI: huggingface-cli download nvidia/Alpamayo-R1-10B
9. Run Alpamayo VLA inference on VP outputs (NEW):
   HF_TOKEN="hf_xxx" bash scripts/build_and_run.sh
   # Automatically discovers latest VP output folder via gcloud storage ls

FULL END-TO-END (alternative):
   # Run all stages (SAM2 â†’ VideoPainter â†’ Alpamayo) with a single command:
   RUN_ID=001 HF_TOKEN="hf_xxx" bash scripts/build_and_run.sh
   # From the repository root â€” builds all 4 containers, pushes to Artifact
   # Registry, and submits master_pipeline_wf which chains all 3 stages.

Random Seeds:
  - PyTorch: set_seed(42) in training scripts
  - Numpy: np.random.seed(42)
  - Python: random.seed(42)

Key Configuration Updates:
  - Learning rate reduced to 1e-5 for better stability
  - Gradient accumulation of 4 for effective batch size of 4
  - Epoch-based training instead of fixed steps
  - More frequent checkpointing (every 100 steps)
  - Automatic timestamp-based run IDs for traceability
  - Master pipeline shared run_id across all stages (NEW)
  - Automated VP output discovery for Alpamayo (NEW)

================================================================================
                           CONCLUSIONS
================================================================================

SUMMARY OF CONTRIBUTIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
This master thesis successfully implemented a complete end-to-end pipeline for
automated lane marking generation, editing, and downstream impact assessment
in autonomous vehicle footage.

Key Achievements:
1. âœ… Integrated FluxFill image inpainting with VideoPainter framework
2. âœ… Implemented VLM-guided iterative refinement for quality control
3. âœ… Created automated SAM2-based road segmentation pipeline
4. âœ… Built large-scale dataset generation system (10,000+ samples)
5. âœ… Developed semantic attribute filtering for dataset curation
6. âœ… Trained domain-specific LoRA models for lane marking generation
7. âœ… Deployed on cloud infrastructure with HLX workflow orchestration
8. âœ… Integrated Alpamayo-R1-10B VLA for trajectory impact assessment (NEW)
9. âœ… Created master pipeline orchestrator chaining SAM2â†’VPâ†’Alpamayo (NEW)
10. âœ… Implemented automated per-video quality metrics (PSNR/SSIM/LPIPS/CLIP) (NEW)
11. âœ… Built multi-instruction editing support for diverse lane variants (NEW)

TECHNICAL IMPACT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- First integration of FluxFill with video inpainting frameworks
- Novel VLM-based quality evaluation loop for inpainting
- Automated dataset generation from autonomous vehicle data
- Scalable cloud deployment with efficient GCS integration
- Reusable components for future video editing research
- First closed-loop pipeline connecting video generation to VLA trajectory
  prediction for measuring downstream autonomous driving impact (NEW)
- Automated numerical quality assessment (PSNR, SSIM, LPIPS, CLIP Score,
  Temporal Consistency) for generated video frames (NEW)
- minADE trajectory metric for quantifying VLA perception shift (NEW)
- Master orchestrator enabling reproducible end-to-end runs with shared
  run identifiers across 4 containerised stages (NEW)

FUTURE WORK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Multi-lane handling for complex intersections
2. Nighttime and adverse weather robustness
3. Real-time inference optimization
4. Larger LoRA training datasets (50,000+ samples)
5. Multi-modal control (text + sketch + reference image)
6. Temporal consistency improvements for longer videos
7. Multi-camera VLA evaluation (all 6 nuPlan cameras simultaneously) (NEW)
8. Comparative VLA study (Alpamayo vs. other driving foundation models) (NEW)
9. Trajectory-conditioned video re-generation feedback loop (NEW)
10. Fine-tuning VLA on domain-shifted synthetic lane data (NEW)

================================================================================
                           END OF DOCUMENT
================================================================================

Last Updated: February 17, 2026

Key Recent Updates:
  - Added Alpamayo-R1-10B VLA trajectory prediction stage (NEW)
  - Created master pipeline orchestrator (workflow_master.py) (NEW)
  - Multi-instruction VideoPainter editing (5 lane variants) (NEW)
  - Per-video numerical evaluation metrics (PSNR/SSIM/LPIPS/CLIP) (NEW)
  - chunks:// URI scheme for SAM2 chunk-based input (NEW)
  - 4 Docker containers (Master + SAM2 + VP + Alpamayo) (NEW)
  - Trajectory overlay visualization with BEV inset (NEW)
  - minADE trajectory metric for VLA impact assessment (NEW)
  - Improved training hyperparameters for better stability
  - Epoch-based training (5 epochs default, configurable)
  - Automatic timestamp-based checkpoint naming
  - Enhanced VLM refinement control (10 iterations default)
  - Configurable inpainting strength parameter
  - Gradient accumulation for effective batch size of 4
  - More frequent checkpointing (every 100 steps)
  - Automatic checkpoint timestamp extraction in inference pipeline

For questions or collaboration:
Email: [CONTACT_EMAIL]
GitHub: [Repository Link]

================================================================================
